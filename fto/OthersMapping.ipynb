{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as Wn\n",
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define stops words list using ntk and others libraries\n",
    "stop_words = list(get_stop_words('en'))         #About 900 stopwords\n",
    "nltk_stop_words = list(stopwords.words('english')) #About 150 stopwords\n",
    "stop_words.extend(nltk_stop_words)\n",
    "\n",
    "annotations_marco_1000_csv_path = \"inputs\\\\1000annotazioni.csv\"\n",
    "states_quantitative_analysis_with_extend_lesk_csv_path = \"outputs\\\\states_quantitative_analysis_with_extend_lesk.csv\"\n",
    "events_quantitative_analysis_with_extend_lesk_csv_path = \"outputs\\\\events_quantitative_analysis_with_extend_lesk.csv\"\n",
    "states_quantitative_analysis_groupby_lemmas_with_extend_lesk_csv_path = \"outputs\\\\states_quantitative_analysis_groupby_lemmas_with_extend_lesk.csv\"\n",
    "events_quantitative_analysis_groupby_lemmas_with_extend_lesk_csv_path = \"outputs\\\\events_quantitative_analysis_groupby_lemmas_with_extend_lesk.csv\"\n",
    "\n",
    "wn_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commons Functions and Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lesk() :\n",
    "    def __init__(self, word, context, pos=None, synsets=None) :\n",
    "        self.sense = self.run(word, context, pos, synsets)\n",
    "\n",
    "    # This function compute synset witc have greater frequence.\n",
    "    def computedMostFrequentSense(self, word, pos=None, synsets=None) :\n",
    "        max_freq = 0\n",
    "        sense = None\n",
    "\n",
    "        if synsets is None:\n",
    "            synsets = Wn.synsets(word)\n",
    "        if pos:\n",
    "            synsets = [ss for ss in synsets if str(ss.pos()) == pos]\n",
    "\n",
    "        for synset in synsets :\n",
    "            freq = 0  \n",
    "            for lemma in synset.lemmas() :\n",
    "                freq +=lemma.count()\n",
    "            if freq > max_freq :\n",
    "                max_freq = freq\n",
    "                sense = synset\n",
    "        return sense\n",
    "\n",
    "    def getDefinitionExamples(self, sense) : \n",
    "            examples = []\n",
    "            if len(sense.examples()) > 0 :\n",
    "                for example in sense.examples() :\n",
    "                    examples += nltk.word_tokenize(example)\n",
    "            return [x for x in (nltk.word_tokenize(sense.definition()) + examples + sense.lemma_names()) if x not in stop_words]\n",
    "\n",
    "    def computedOverlap(self, signature, context) :\n",
    "        return len(list(set(signature).intersection(set(context))))\n",
    "        \n",
    "    \n",
    "    def run(self, word, context, pos, synsets) :\n",
    "        max_overlap = 0\n",
    "        best_sense = self.computedMostFrequentSense(word, pos, synsets)\n",
    "        senses = Wn.synsets(word) if synsets is None else synsets\n",
    "        if pos:\n",
    "            senses = [ss for ss in senses if str(ss.pos()) == pos]\n",
    "        context_ = [x for x in context if x not in stop_words]\n",
    "        for sense in  senses:\n",
    "            signature = self.getDefinitionExamples(sense)\n",
    "            overlap = self.computedOverlap(signature, context_)\n",
    "            if overlap > max_overlap :\n",
    "                max_overlap = overlap\n",
    "                best_sense = sense\n",
    "        return best_sense\n",
    "\n",
    "    def get(self) :\n",
    "        return self.sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of annotations over 1000 sentences:  1795\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "verbs = {\n",
    "    'states': {},\n",
    "    'events': {}\n",
    "}\n",
    "df_marco = pd.read_csv(annotations_marco_1000_csv_path).replace(np.nan, '', regex=True)\n",
    "print(\"Number of annotations over 1000 sentences: \", df_marco.shape[0])\n",
    "\n",
    "for index, row in df_marco.iterrows() :\n",
    "    state = str(row[\"STATE\"]).strip().lower()\n",
    "    event = str(row[\"EVENT\"]).strip().lower()\n",
    "    rep_event = str(row[\"REP-EVENT\"]).strip().lower()\n",
    "    asp_event = str(row[\"ASP-EVENT\"]).strip().lower()\n",
    "    location = str(row[\"LOC\"]).strip()\n",
    "    orgaization = str(row[\"ORG\"]).strip()\n",
    "    text = str(row['text'])\n",
    "    is_verb = True\n",
    "    key_verbs = verbs['states'] if len(event.strip()) == 0 else verbs['events']\n",
    "    verb_state_ = state if len(event.strip()) == 0 else event\n",
    "    doc1 = nlp(location)\n",
    "    doc2 = nlp(orgaization)\n",
    "    entities1 = [ent.label_ for ent in doc1.ents]\n",
    "    entities2 = [ent.label_ for ent in doc2.ents]\n",
    "    verb_states = [w for w in word_tokenize(verb_state_) if not w in stop_words]\n",
    "    for verb_state in verb_states :\n",
    "        best_sense = Lesk(verb_state, word_tokenize(text), pos=Wn.VERB).get()\n",
    "        if best_sense is None :\n",
    "            best_sense = Lesk(verb_state, word_tokenize(text)).get()\n",
    "            is_verb = False\n",
    "        definition = best_sense.definition() if best_sense is not None else verb_state\n",
    "        best_sense = best_sense.name() if best_sense is not None else verb_state\n",
    "        if best_sense in key_verbs :\n",
    "            key_verbs[best_sense]['occorrences'] += 1\n",
    "            key_verbs[best_sense]['rep_event'] += 1 if len(rep_event) > 0 else 0\n",
    "            key_verbs[best_sense]['asp_event'] += 1 if len(asp_event) > 0 else 0 \n",
    "            key_verbs[best_sense]['location'] += 1 if \"GPE\" in entities1 else 0\n",
    "            key_verbs[best_sense]['organization'] += 1 if \"ORG\" in entities2 else 0\n",
    "            if is_verb :\n",
    "                key_verbs[best_sense]['targets'].add(wn_lemmatizer.lemmatize(verb_state, 'v'))\n",
    "            else : \n",
    "                key_verbs[best_sense]['targets'].add(wn_lemmatizer.lemmatize(verb_state))\n",
    "        else :\n",
    "            key_verbs[best_sense] = {}\n",
    "            key_verbs[best_sense]['occorrences'] = 1\n",
    "            key_verbs[best_sense]['definition'] = definition\n",
    "            key_verbs[best_sense]['rep_event'] = 1 if len(rep_event) > 0 else 0\n",
    "            key_verbs[best_sense]['asp_event'] = 1 if len(asp_event) > 0 else 0\n",
    "            key_verbs[best_sense]['location'] = 1 if \"GPE\" in entities1 else 0\n",
    "            key_verbs[best_sense]['organization'] = 1 if \"ORG\" in entities2 else 0\n",
    "            if is_verb :\n",
    "                key_verbs[best_sense]['targets'] = set([wn_lemmatizer.lemmatize(verb_state, 'v')])\n",
    "            else :\n",
    "                key_verbs[best_sense]['targets'] = set([wn_lemmatizer.lemmatize(verb_state)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of annotations (STATE) over 1000 sentences: 677 , sia 37.71587743732591 %.\n",
      "Number of annotations (EVENT) over 1000 sentences: 853 , sia 47.5208913649025 %.\n"
     ]
    }
   ],
   "source": [
    "df_states_quant_analysis = pd.DataFrame(columns=[\n",
    "    'state (WORDNET SENSE IF EXIST)', \n",
    "    'occorrences', \n",
    "    'definition', \n",
    "    'rep_event', \n",
    "    'asp_event', \n",
    "    'location', \n",
    "    'organization', \n",
    "    'targets'\n",
    "])\n",
    "for key, value in verbs['states'].items() :\n",
    "    value_copy = value.copy()\n",
    "    value_copy['state (WORDNET SENSE IF EXIST)'] = key\n",
    "    value_copy['targets'] = \", \".join(value['targets'])\n",
    "    df_states_quant_analysis = df_states_quant_analysis.append(value_copy, ignore_index=True)\n",
    "df_states_quant_analysis = df_states_quant_analysis.sort_values(by='occorrences', ascending=False)\n",
    "print(\n",
    "    \"Number of annotations (STATE) over 1000 sentences:\", \n",
    "    df_states_quant_analysis['occorrences'].sum(),\n",
    "    \", sia\",\n",
    "    (df_states_quant_analysis['occorrences'].sum() * 100) / df_marco.shape[0],\n",
    "    \"%.\"\n",
    ")\n",
    "df_states_quant_analysis.to_csv(states_quantitative_analysis_with_extend_lesk_csv_path, index=False)\n",
    "\n",
    "df_events_quant_analysis = pd.DataFrame(columns=[\n",
    "    'event (WORDNET SENSE IF EXIST)', \n",
    "    'occorrences', \n",
    "    'definition', \n",
    "    'rep_event', \n",
    "    'asp_event', \n",
    "    'location', \n",
    "    'organization', \n",
    "    'targets'\n",
    "])\n",
    "for key, value in verbs['events'].items() :\n",
    "    value_copy = value.copy()\n",
    "    value_copy['event (WORDNET SENSE IF EXIST)'] = key\n",
    "    value_copy['targets'] = \", \".join(value['targets'])\n",
    "    df_events_quant_analysis = df_events_quant_analysis.append(value_copy, ignore_index=True)\n",
    "df_events_quant_analysis = df_events_quant_analysis.sort_values(by='occorrences', ascending=False)\n",
    "print(\n",
    "    \"Number of annotations (EVENT) over 1000 sentences:\", \n",
    "    df_events_quant_analysis['occorrences'].sum(),\n",
    "    \", sia\",\n",
    "    (df_events_quant_analysis['occorrences'].sum() * 100) / df_marco.shape[0],\n",
    "    \"%.\"\n",
    ")\n",
    "df_events_quant_analysis.to_csv(events_quantitative_analysis_with_extend_lesk_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg1 = df_states_quant_analysis.groupby('targets').agg({\n",
    "    'state (WORDNET SENSE IF EXIST)': ' ## '.join, \n",
    "    'occorrences': sum, \n",
    "    'definition': ' ## '.join, \n",
    "    'rep_event': sum, \n",
    "    'asp_event': sum, \n",
    "    'location': sum, \n",
    "    'organization': sum,\n",
    "    'targets': ' ## '.join\n",
    "})\n",
    "df_agg1 = df_agg1.sort_values(by='occorrences', ascending=False)\n",
    "df_agg1.to_csv(states_quantitative_analysis_groupby_lemmas_with_extend_lesk_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg2 = df_events_quant_analysis.groupby('targets').agg({\n",
    "    'event (WORDNET SENSE IF EXIST)': ' ## '.join, \n",
    "    'occorrences': sum, \n",
    "    'definition': ' ## '.join, \n",
    "    'rep_event': sum, \n",
    "    'asp_event': sum, \n",
    "    'location': sum, \n",
    "    'organization': sum,\n",
    "    'targets': ' ## '.join\n",
    "})\n",
    "df_agg2 = df_agg2.sort_values(by='occorrences', ascending=False)\n",
    "df_agg2.to_csv(events_quantitative_analysis_groupby_lemmas_with_extend_lesk_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('receive.v.02') receive a specified treatment (abstract)\n",
      "Synset('receive.v.02') receive a specified treatment (abstract)\n",
      "Synset('prevail.v.02') be valid, applicable, or true\n",
      "Synset('receive.v.02') receive a specified treatment (abstract)\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Ramachandran, whose father wanted him to become a physician rather than a researcher, obtained an M.B.B.S. from Stanley Medical College in Chennai, India.\"\n",
    "sense = lesk(word_tokenize(sentence), 'obtained', pos=Wn.VERB)\n",
    "print(sense, sense.definition())\n",
    "\n",
    "sentence = \"He received the Kerala Sahitya Academi award in the `poetry' section for his collection, Nellickal Muraleedharante Kavithakal in 2004.\"\n",
    "sense = lesk(word_tokenize(sentence), 'obtained', pos=Wn.VERB)\n",
    "print(sense, sense.definition())\n",
    "\n",
    "sentence = \"Ramachandran, whose father wanted him to become a physician rather than a researcher, obtained an M.B.B.S. from Stanley Medical College in Chennai, India.\"\n",
    "sense = Lesk('obtained', word_tokenize(sentence), pos=Wn.VERB).get()\n",
    "print(sense, sense.definition())\n",
    "\n",
    "sentence = \"He received the Kerala Sahitya Academi award in the `poetry' section for his collection, Nellickal Muraleedharante Kavithakal in 2004.\"\n",
    "sense = Lesk('obtained', word_tokenize(sentence), pos=Wn.VERB).get()\n",
    "print(sense, sense.definition())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9c0098ef03c01875785b14fb83d5161d4a7df326e3b00d12d4d2605823f1c481"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
