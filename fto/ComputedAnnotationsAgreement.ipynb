{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Customer Variables\n",
    "annotations_ousmane_200_json_path = \"inputs\\\\annotations-ousmane-200.json\" \n",
    "annotations_rossana_200_json_path = \"inputs\\\\rossana_annotations.json\" \n",
    "annotations_ousmane_200_csv_path = \"inputs\\\\annotations-ousmane-200.csv\"\n",
    "annotations_marco_1000_csv_path = \"inputs\\\\1000annotazioni.csv\"\n",
    "annotations_marco_200_csv_path = \"inputs\\\\annotations-marco-200.csv\"\n",
    "annotations_rossana_200_csv_path = \"inputs\\\\annotations-rossana-200.csv\"\n",
    "annotations_matched_200_csv_path = \"outputs\\\\annotations-matched-200.csv\"\n",
    "annotations_contains_matched_200_csv_path = \"outputs\\\\annotations_contains-matched-200.csv\"\n",
    "states_quantitative_analysis_csv_path = \"outputs\\\\states_quantitative_analysis.csv\"\n",
    "events_quantitative_analysis_csv_path = \"outputs\\\\events_quantitative_analysis.csv\"\n",
    "states_quantitative_analysis_groupby_lemmas = \"outputs\\\\states_quantitative_analysis_groupby_lemmas.csv\"\n",
    "events_quantitative_analysis_groupby_lemmas = \"outputs\\\\events_quantitative_analysis_groupby_lemmas.csv\"\n",
    "test_csv_path = \"outputs\\\\test.csv\"\n",
    "temp_computing_path = \"outputs\\\\temp_computing.csv\"\n",
    "exact_agreement_columns = [\"author\", \"sent_id\", \"text\", \"TIME\", \"WRITER-AG\", \"EVENT\", \"ORG\", \"LOC\", \"ASP-EVENT\", \"STATE\", \"WRITER-PA\", \"REP-EVENT\"]\n",
    "soft_agreement_columns = [\"author\", \"sent_id\", \"text\", \"EVENT\", \"STATE\"]\n",
    "merge_by_columns = [\"author\", \"sent_id\", \"text\"]\n",
    "element_types_without_events_and_states = [\"WRITER-AG\", \"ORG\", \"LOC\", \"ASP-EVENT\", \"WRITER-PA\", \"REP-EVENT\"]\n",
    "\n",
    "## Libraries Variables\n",
    "stop_words = list(get_stop_words('en'))         #About 900 stopwords\n",
    "nltk_words = list(stopwords.words('english')) #About 150 stopwords\n",
    "stop_words.extend(nltk_words)\n",
    "wn_lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function convert the annotation file json in a annotation file csv.\n",
    "def annotations_json_to_csv(path_file_json, annotator_name, path_file_csv) :\n",
    "    df = pd.DataFrame()\n",
    "    with open(path_file_json, \"r\", encoding='utf-8') as f:\n",
    "        file_contents = json.load(f)\n",
    "        for item in file_contents:\n",
    "            for annotation in item['annotations']:\n",
    "                container = dict()\n",
    "                container['annotator'] = annotator_name\n",
    "                container['author'] = item['data']['author']\n",
    "                container['sent_id'] = item['data']['sent_id']\n",
    "                container['text'] = item['data']['text']\n",
    "                for v in annotation['result']:\n",
    "                    container[v['value']['labels'][0]] = v['value']['text']\n",
    "                df = df.append(container, ignore_index=True)\n",
    "    df.to_csv(path_file_csv, index=False)\n",
    "\n",
    "# This function verify if string elements of two list are \"contains relations\".\n",
    "# \"contains relations\": for example we have two string str1 and str2, if str1 is sub strinf of str2\n",
    "# or str2 is sub string of str1\n",
    "def checkContainsRelations(items_1, items_2) :\n",
    "    computed = True\n",
    "    if len(items_1) != len(items_2) : return False\n",
    "    for index in range(len(items_1)) : \n",
    "        #computed = computed and (str(items_1[index]) in str(items_2[index]) or str(items_2[index]) in str(items_1[index]))\n",
    "        sub_compute = True\n",
    "        lx = str(items_1[index]).split()\n",
    "        ly = str(items_2[index]).split()\n",
    "        if len(lx) > len(ly) :\n",
    "            lx, ly = swapedItems(lx, ly)\n",
    "        for i in range(len(lx)) :\n",
    "            sub_compute = sub_compute and (lx[i] in ly)\n",
    "            if not sub_compute :\n",
    "                break\n",
    "        computed = computed and sub_compute\n",
    "        if not computed : return False\n",
    "    return computed\n",
    "\n",
    "# Contain relation between two string\n",
    "def checkContainRelation(item_1, item_2) :\n",
    "    computed = True\n",
    "    lx = str(item_1).split()\n",
    "    ly = str(item_2).split()\n",
    "    if len(lx) > len(ly) :\n",
    "        lx, ly = swapedItems(lx, ly)\n",
    "    for i in range(len(lx)) :\n",
    "        computed = computed and (lx[i] in ly)\n",
    "        if not computed :\n",
    "            return False\n",
    "    return computed\n",
    "\n",
    "\n",
    "## This function merge two csv with Pandas\n",
    "def merged_two_csv(path_csv1, path_csv2, how_mode, onColumns) :\n",
    "    df1 = pd.read_csv(path_csv1)\n",
    "    df2 = pd.read_csv(path_csv2)\n",
    "    df = pd.merge(df1, df2, how=how_mode, on=onColumns) \n",
    "    return df\n",
    "\n",
    "def concateItems(item_1, item_2) :\n",
    "    item = \"Value1: \"\n",
    "    item += str(item_1) if str(item_1) != \"nan\" else \"MISSING_VALUE\"\n",
    "    item += \" & Value2: \"\n",
    "    item += str(item_2) if str(item_2) != \"nan\" else \"MISSING_VALUE\"\n",
    "    return item\n",
    "\n",
    "def swapedItems(item_1, item_2) :\n",
    "    temp = item_1\n",
    "    item_1 = item_2\n",
    "    item_2 = temp\n",
    "    return item_1,item_2\n",
    "\n",
    "## \n",
    "def computeContainsRelationMatchByEventOrState(df_annotations1, df_annotations2, output_file_name=\"\") :\n",
    "    df_set_pivot = df_annotations1[[\"author\", \"sent_id\", \"text\"]].drop_duplicates(subset=[\"author\", \"sent_id\", \"text\"])\n",
    "\n",
    "    # Create a matched Dictionnary container\n",
    "    matched_container = {\n",
    "        'Author': [],\n",
    "        'SentenceId': [],\n",
    "        'EVENT': [],\n",
    "        'STATE': [],\n",
    "        'MatchedInfo': [],\n",
    "        'Text': []\n",
    "    }\n",
    "\n",
    "    # Iterate Pivot df and Processing soft matching on Some columns \n",
    "    for _, row in df_set_pivot.iterrows():\n",
    "        author = row[\"author\"]\n",
    "        sent_id = row[\"sent_id\"]\n",
    "        text = row[\"text\"]\n",
    "        df_x = df_annotations1.loc[(df_annotations1['author'] == author) & (df_annotations1['sent_id'] == sent_id) & (df_annotations1['text'] == text)]\n",
    "        df_y = df_annotations2.loc[(df_annotations2['author'] == author) & (df_annotations2['sent_id'] == sent_id) & (df_annotations2['text'] == text)]\n",
    "        if (df_x.shape[0] < df_y.shape[0]) :\n",
    "            df_x, df_y = swapedItems(df_x, df_y)\n",
    "        for _, row_x in df_x.iterrows() :\n",
    "            items_1 = [row_x[\"EVENT\"], row_x[\"STATE\"]]\n",
    "            for index_y, row_y in df_y.iterrows() :\n",
    "                items_2 = [row_y[\"EVENT\"], row_y[\"STATE\"]]\n",
    "                if items_1 == items_2 : \n",
    "                    matched_container['Author'].append(author)\n",
    "                    matched_container['SentenceId'].append(sent_id)\n",
    "                    matched_container['Text'].append(text)\n",
    "                    matched_container['EVENT'].append(row_x[\"EVENT\"])\n",
    "                    matched_container['STATE'].append(row_x[\"STATE\"])\n",
    "                    matched_container['MatchedInfo'].append(\"Exact Match\")\n",
    "                    df_y = df_y.drop(index_y)\n",
    "                    break\n",
    "                elif checkContainsRelations(items_1, items_2) :\n",
    "                    matched_container['Author'].append(author)\n",
    "                    matched_container['SentenceId'].append(sent_id)\n",
    "                    matched_container['Text'].append(text)\n",
    "                    if str(row_x[\"EVENT\"]) != \"nan\" or str(row_y[\"EVENT\"]) != \"nan\" : \n",
    "                        matched_container['EVENT'].append(concateItems(row_x[\"EVENT\"],row_y[\"EVENT\"]))\n",
    "                    else : matched_container['EVENT'].append(\"\")\n",
    "                    if str(row_x[\"STATE\"]) != \"nan\" or str(row_y[\"STATE\"]) != \"nan\" : \n",
    "                        matched_container['STATE'].append(concateItems(row_x[\"STATE\"], row_y[\"STATE\"]))\n",
    "                    else : matched_container['STATE'].append(\"\")\n",
    "                    matched_container['MatchedInfo'].append(\"Contains Relation Match\")\n",
    "                    df_y = df_y.drop(index_y)\n",
    "                    break\n",
    "\n",
    "    # Convert the dictionary into DataFrame\n",
    "    df_contains_matched = pd.DataFrame(matched_container, columns = ['EVENT', 'STATE', 'MatchedInfo', 'SentenceId', 'Text'])\n",
    "    df_contains_matched.replace(np.nan, '', regex=True)\n",
    "    if output_file_name != \"\" and isinstance(output_file_name, str) : \n",
    "        df_contains_matched.to_csv(output_file_name, index=False)\n",
    "    return df_contains_matched\n",
    "\n",
    "## This function take two annotations Dataframe and compute the Agreement by Category\n",
    "def computeContainsRelationMatchByElementType(df_annotations1, df_annotations2, element_type, output_file_name) : \n",
    "    \n",
    "    # Create a matched Dictionnary container\n",
    "    matched_container = {\n",
    "        'Author': [],\n",
    "        'SentenceId': [],\n",
    "        element_type: [],\n",
    "        'MatchedInfo': [],\n",
    "        'Text': []\n",
    "    }\n",
    "\n",
    "    if element_type not in element_types_without_events_and_states :\n",
    "        raise Exception(\"Element Type not exist or has type EVENT or STATE!\")\n",
    "\n",
    "    # Compute Pivot Set\n",
    "    df_set_pivot = df_annotations1[[\"author\", \"sent_id\", \"text\"]].drop_duplicates(subset=[\"author\", \"sent_id\", \"text\"])\n",
    "\n",
    "    for _, row in df_set_pivot.iterrows() :\n",
    "        author = row[\"author\"]\n",
    "        sent_id = row[\"sent_id\"]\n",
    "        text = row[\"text\"]\n",
    "        df_x = df_annotations1.loc[(df_annotations1['author'] == author) & (df_annotations1['sent_id'] == sent_id) & (df_annotations1['text'] == text)]\n",
    "        df_y = df_annotations2.loc[(df_annotations2['author'] == author) & (df_annotations2['sent_id'] == sent_id) & (df_annotations2['text'] == text)]\n",
    "        if (df_x.shape[0] < df_y.shape[0]) :\n",
    "            df_x, df_y = swapedItems(df_x, df_y)\n",
    "        for _, row_x in df_x.iterrows() :\n",
    "            items_1 = [row_x[element_type]]\n",
    "            for index_y, row_y in df_y.iterrows() :\n",
    "                items_2 = [row_y[element_type]]\n",
    "                if items_1 == items_2 : \n",
    "                    matched_container['Author'].append(author)\n",
    "                    matched_container['SentenceId'].append(sent_id)\n",
    "                    matched_container['Text'].append(text)\n",
    "                    matched_container[element_type].append(row_x[element_type])\n",
    "                    matched_container['MatchedInfo'].append(\"Exact Match\")\n",
    "                    df_y = df_y.drop(index_y)\n",
    "                    break\n",
    "                elif checkContainsRelations(items_1, items_2) :\n",
    "                    matched_container['Author'].append(author)\n",
    "                    matched_container['SentenceId'].append(sent_id)\n",
    "                    matched_container['Text'].append(text)\n",
    "                    if str(row_x[element_type]) != \"nan\" or str(row_y[element_type]) != \"nan\" : \n",
    "                        matched_container[element_type].append(concateItems(row_x[element_type],row_y[element_type]))\n",
    "                    else : matched_container[element_type].append(\"\")\n",
    "                    matched_container['MatchedInfo'].append(\"Contains Relation Match\")\n",
    "                    df_y = df_y.drop(index_y)\n",
    "                    break\n",
    "        \n",
    "    # Convert the dictionary into DataFrame\n",
    "    df_contains_matched = pd.DataFrame(matched_container, columns = [element_type, 'MatchedInfo', 'SentenceId', 'Text'])\n",
    "    df_contains_matched.replace(np.nan, '', regex=True)\n",
    "    df_contains_matched.to_csv(output_file_name, index=False)\n",
    "    return df_contains_matched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trnasform Annotions file JSON to file CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotations_json_to_csv(annotations_rossana_200_json_path, \"rossana\", annotations_rossana_200_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute exact Agreement \n",
    "### Output file => \"outputs\\\\annotations-matched-200.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exact_matched = merged_two_csv(annotations_ousmane_200_csv_path, annotations_marco_1000_csv_path, \"inner\", exact_agreement_columns)\n",
    "df_exact_matched.to_csv(annotations_matched_200_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute soft Agreement (columns values are contains relations)\n",
    "### Output file => \"outputs\\\\annotations_contains-matched-200.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Both df of annotatore and  make Pivot df\n",
    "df_ousmane = pd.read_csv(annotations_ousmane_200_csv_path)\n",
    "df_marco_200 = pd.read_csv(annotations_marco_200_csv_path)\n",
    "df_contains_matched = computeContainsRelationMatchByEventOrState(df_ousmane, df_marco_200, annotations_contains_matched_200_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-------------------------+----------------------+---------------------+-------------+-------------------------+----------------+\n",
      "|         GOLD ANNOTATIONS        |    Intersection Mode    | Precision Evaluation |  Recall Evaluation  | Exact Match | Contains Relation Match |  Type Element  |\n",
      "+---------------------------------+-------------------------+----------------------+---------------------+-------------+-------------------------+----------------+\n",
      "|  Annoations for Fist Annotator  |       Exact Match       |  0.3333333333333333  | 0.27823691460055094 |     101     |            0            |  ALL ELEMENT   |\n",
      "|  Annoations for Fist Annotator  | Contains Relation Match |  0.6996699669966997  |  0.5840220385674931 |     205     |            7            | EVENT or STATE |\n",
      "| Annoations for Second Annotator |       Exact Match       | 0.27823691460055094  |  0.3333333333333333 |     101     |            0            |  ALL ELEMENT   |\n",
      "| Annoations for Second Annotator | Contains Relation Match |  0.5840220385674931  |  0.6996699669966997 |     205     |            7            | EVENT or STATE |\n",
      "+---------------------------------+-------------------------+----------------------+---------------------+-------------+-------------------------+----------------+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t = PrettyTable(['GOLD ANNOTATIONS', 'Intersection Mode', 'Precision Evaluation', 'Recall Evaluation', 'Exact Match', 'Contains Relation Match', 'Type Element'])\n",
    "\n",
    "exaxt_match = df_contains_matched[df_contains_matched.MatchedInfo == 'Exact Match'].shape[0]\n",
    "contains_relation_match = df_contains_matched[df_contains_matched.MatchedInfo == 'Contains Relation Match'].shape[0]\n",
    "\n",
    "precision = (df_exact_matched.shape[0])/(df_ousmane.shape[0])\n",
    "recall = (df_exact_matched.shape[0])/(df_marco_200.shape[0])\n",
    "t.add_row(['Annoations for Fist Annotator', 'Exact Match', precision, recall, df_exact_matched.shape[0], 0, 'ALL ELEMENT'])\n",
    "\n",
    "precision = (df_contains_matched.shape[0])/(df_ousmane.shape[0])\n",
    "recall = (df_contains_matched.shape[0])/(df_marco_200.shape[0])\n",
    "t.add_row(['Annoations for Fist Annotator', 'Contains Relation Match', precision, recall, exaxt_match, contains_relation_match, 'EVENT or STATE'])\n",
    "\n",
    "precision = (df_exact_matched.shape[0])/(df_marco_200.shape[0])\n",
    "recall = (df_exact_matched.shape[0])/(df_ousmane.shape[0])\n",
    "t.add_row(['Annoations for Second Annotator', 'Exact Match', precision, recall, df_exact_matched.shape[0], 0, 'ALL ELEMENT'])\n",
    "\n",
    "precision = (df_contains_matched.shape[0])/(df_marco_200.shape[0])\n",
    "recall = (df_contains_matched.shape[0])/(df_ousmane.shape[0])\n",
    "t.add_row(['Annoations for Second Annotator', 'Contains Relation Match', precision, recall, exaxt_match, contains_relation_match, 'EVENT or STATE'])\n",
    "\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paramatize Computing Precision and Recall on Elements with types: \"WRITER-AG\", \"ORG\", \"LOC\", \"ASP-EVENT\", \"WRITER-PA\", \"REP-EVENT\".\n",
    "### Using the Soft Agreement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-------------------------+----------------------+--------------------+-------------+-------------------------+--------------+\n",
      "|         GOLD ANNOTATIONS        |    Intersection Mode    | Precision Evaluation | Recall Evaluation  | Exact Match | Contains Relation Match | Type Element |\n",
      "+---------------------------------+-------------------------+----------------------+--------------------+-------------+-------------------------+--------------+\n",
      "|  Annoations for Fist Annotator  | Contains Relation Match |  0.858085808580858   | 0.7162534435261708 |     257     |            3            |  WRITER-AG   |\n",
      "| Annoations for Second Annotator | Contains Relation Match |  0.7162534435261708  | 0.858085808580858  |     257     |            3            |  WRITER-AG   |\n",
      "|                                 |                         |                      |                    |             |                         |              |\n",
      "|  Annoations for Fist Annotator  | Contains Relation Match |  0.7920792079207921  | 0.6611570247933884 |     220     |            20           |     ORG      |\n",
      "| Annoations for Second Annotator | Contains Relation Match |  0.6611570247933884  | 0.7920792079207921 |     220     |            20           |     ORG      |\n",
      "|                                 |                         |                      |                    |             |                         |              |\n",
      "|  Annoations for Fist Annotator  | Contains Relation Match |  0.8415841584158416  | 0.7024793388429752 |     241     |            14           |     LOC      |\n",
      "| Annoations for Second Annotator | Contains Relation Match |  0.7024793388429752  | 0.8415841584158416 |     241     |            14           |     LOC      |\n",
      "|                                 |                         |                      |                    |             |                         |              |\n",
      "|  Annoations for Fist Annotator  | Contains Relation Match |  0.9174917491749175  | 0.7658402203856749 |     278     |            0            |  ASP-EVENT   |\n",
      "| Annoations for Second Annotator | Contains Relation Match |  0.7658402203856749  | 0.9174917491749175 |     278     |            0            |  ASP-EVENT   |\n",
      "|                                 |                         |                      |                    |             |                         |              |\n",
      "|  Annoations for Fist Annotator  | Contains Relation Match |  0.8976897689768977  | 0.7493112947658402 |     270     |            2            |  WRITER-PA   |\n",
      "| Annoations for Second Annotator | Contains Relation Match |  0.7493112947658402  | 0.8976897689768977 |     270     |            2            |  WRITER-PA   |\n",
      "|                                 |                         |                      |                    |             |                         |              |\n",
      "|  Annoations for Fist Annotator  | Contains Relation Match |  0.9471947194719472  | 0.790633608815427  |     287     |            0            |  REP-EVENT   |\n",
      "| Annoations for Second Annotator | Contains Relation Match |  0.790633608815427   | 0.9471947194719472 |     287     |            0            |  REP-EVENT   |\n",
      "|                                 |                         |                      |                    |             |                         |              |\n",
      "+---------------------------------+-------------------------+----------------------+--------------------+-------------+-------------------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "t = PrettyTable(['GOLD ANNOTATIONS', 'Intersection Mode', 'Precision Evaluation', 'Recall Evaluation', 'Exact Match', 'Contains Relation Match', 'Type Element'])\n",
    "for type_element in element_types_without_events_and_states :\n",
    "    df_temp = computeContainsRelationMatchByElementType(df_ousmane, df_marco_200, type_element, temp_computing_path)\n",
    "\n",
    "    exaxt_match = df_temp[df_temp.MatchedInfo == 'Exact Match'].shape[0]\n",
    "    contains_relation_match = df_temp[df_temp.MatchedInfo == 'Contains Relation Match'].shape[0]\n",
    "\n",
    "    precision = (df_temp.shape[0])/(df_ousmane.shape[0])\n",
    "    recall = (df_temp.shape[0])/(df_marco_200.shape[0])\n",
    "    t.add_row(['Annoations for Fist Annotator', 'Contains Relation Match', precision, recall, exaxt_match, contains_relation_match, type_element])\n",
    "\n",
    "    precision = (df_temp.shape[0])/(df_marco_200.shape[0])\n",
    "    recall = (df_temp.shape[0])/(df_ousmane.shape[0])\n",
    "    t.add_row(['Annoations for Second Annotator', 'Contains Relation Match', precision, recall, exaxt_match, contains_relation_match, type_element])\n",
    "    \n",
    "    t.add_row(['', '', '', '', '', '', ''])\n",
    "\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-------------------------+----------------------+---------------------+-------------+-------------------------+----------------+\n",
      "|         GOLD ANNOTATIONS        |    Intersection Mode    | Precision Evaluation |  Recall Evaluation  | Exact Match | Contains Relation Match |  Type Element  |\n",
      "+---------------------------------+-------------------------+----------------------+---------------------+-------------+-------------------------+----------------+\n",
      "|  Annoations for Fist Annotator  |       Exact Match       | 0.27485380116959063  |  0.2865853658536585 |      94     |            0            |  ALL ELEMENT   |\n",
      "|  Annoations for Fist Annotator  | Contains Relation Match |  0.6842105263157895  |  0.7134146341463414 |      0      |           234           | EVENT or STATE |\n",
      "| Annoations for Second Annotator |       Exact Match       |  0.2865853658536585  | 0.27485380116959063 |      94     |            0            |  ALL ELEMENT   |\n",
      "| Annoations for Second Annotator | Contains Relation Match |  0.7134146341463414  |  0.6842105263157895 |      0      |           234           | EVENT or STATE |\n",
      "+---------------------------------+-------------------------+----------------------+---------------------+-------------+-------------------------+----------------+\n"
     ]
    }
   ],
   "source": [
    "df_marco = pd.read_csv(annotations_marco_1000_csv_path).replace(np.nan, '', regex=True)\n",
    "df_rossana = pd.read_csv(annotations_rossana_200_csv_path)\n",
    "df_marco_200_second = pd.DataFrame(columns=list(df_rossana.columns))\n",
    "df_set_pivot = df_rossana[[\"author\", \"sent_id\", \"text\"]].drop_duplicates(subset=[\"author\", \"sent_id\", \"text\"])\n",
    "for _, row in df_set_pivot.iterrows() :\n",
    "    author = row[\"author\"]\n",
    "    sent_id = row[\"sent_id\"]\n",
    "    text = row[\"text\"]\n",
    "    df_ = df_marco.loc[(df_marco['author'] == author) & (df_marco['sent_id'] == sent_id) & (df_marco['text'] == text)]\n",
    "    for _, row_ in df_.iterrows() :\n",
    "        df_marco_200_second = df_marco_200_second.append(row_)\n",
    "df_marco_200_second.to_csv(test_csv_path)\n",
    "\n",
    "t = PrettyTable(['GOLD ANNOTATIONS', 'Intersection Mode', 'Precision Evaluation', 'Recall Evaluation', 'Exact Match', 'Contains Relation Match', 'Type Element'])\n",
    "\n",
    "df_exact_matched = merged_two_csv(test_csv_path, annotations_rossana_200_csv_path, \"inner\", exact_agreement_columns)\n",
    "df_contains_matched = computeContainsRelationMatchByEventOrState(df_marco, df_rossana)\n",
    "\n",
    "exaxt_match = df_contains_matched[df_contains_matched.MatchedInfo == 'Exact Match'].shape[0]\n",
    "contains_relation_match = df_contains_matched[df_contains_matched.MatchedInfo == 'Contains Relation Match'].shape[0]\n",
    "\n",
    "precision = (df_exact_matched.shape[0])/(df_marco_200_second.shape[0])\n",
    "recall = (df_exact_matched.shape[0])/(df_rossana.shape[0])\n",
    "t.add_row(['Annoations for Fist Annotator', 'Exact Match', precision, recall, df_exact_matched.shape[0], 0, 'ALL ELEMENT'])\n",
    "\n",
    "precision = (df_contains_matched.shape[0])/(df_marco_200_second.shape[0])\n",
    "recall = (df_contains_matched.shape[0])/(df_rossana.shape[0])\n",
    "t.add_row(['Annoations for Fist Annotator', 'Contains Relation Match', precision, recall, exaxt_match, contains_relation_match, 'EVENT or STATE'])\n",
    "\n",
    "precision = (df_exact_matched.shape[0])/(df_rossana.shape[0])\n",
    "recall = (df_exact_matched.shape[0])/(df_marco_200_second.shape[0])\n",
    "t.add_row(['Annoations for Second Annotator', 'Exact Match', precision, recall, df_exact_matched.shape[0], 0, 'ALL ELEMENT'])\n",
    "\n",
    "precision = (df_contains_matched.shape[0])/(df_rossana.shape[0])\n",
    "recall = (df_contains_matched.shape[0])/(df_marco_200_second.shape[0])\n",
    "t.add_row(['Annoations for Second Annotator', 'Contains Relation Match', precision, recall, exaxt_match, contains_relation_match, 'EVENT or STATE'])\n",
    "\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['annotator', 'author', 'sent_id', 'text', 'REP-EVENT', 'STATE', 'LOC', 'ORG', 'WRITER-PA', 'EVENT', 'WRITER-AG', 'TIME', 'ASP-EVENT']\n"
     ]
    }
   ],
   "source": [
    "print(list(df_rossana.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of annotations over 1000 sentences:  1795\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "verbs = {\n",
    "    'states': {},\n",
    "    'events': {}\n",
    "}\n",
    "df_marco = pd.read_csv(annotations_marco_1000_csv_path).replace(np.nan, '', regex=True)\n",
    "print(\"Number of annotations over 1000 sentences: \", df_marco.shape[0])\n",
    "\n",
    "for index, row in df_marco.iterrows() :\n",
    "    state = str(row[\"STATE\"]).strip().lower()\n",
    "    event = str(row[\"EVENT\"]).strip().lower()\n",
    "    rep_event = str(row[\"REP-EVENT\"]).strip().lower()\n",
    "    asp_event = str(row[\"ASP-EVENT\"]).strip().lower()\n",
    "    location = str(row[\"LOC\"]).strip()\n",
    "    orgaization = str(row[\"ORG\"]).strip()\n",
    "    text = str(row['text'])\n",
    "    is_verb = True\n",
    "    key_verbs = verbs['states'] if len(event.strip()) == 0 else verbs['events']\n",
    "    verb_state_ = state if len(event.strip()) == 0 else event\n",
    "    doc1 = nlp(location)\n",
    "    doc2 = nlp(orgaization)\n",
    "    entities1 = [ent.label_ for ent in doc1.ents]\n",
    "    entities2 = [ent.label_ for ent in doc2.ents]\n",
    "    verb_states = [w for w in word_tokenize(verb_state_) if not w in stop_words]\n",
    "    for verb_state in verb_states :\n",
    "        best_sense = lesk(word_tokenize(text), verb_state, pos=wn.VERB)\n",
    "        if best_sense is None :\n",
    "            best_sense = lesk(word_tokenize(text), verb_state)\n",
    "            is_verb = False\n",
    "        definition = best_sense.definition() if best_sense is not None else verb_state\n",
    "        best_sense = best_sense.name() if best_sense is not None else verb_state\n",
    "        if best_sense in key_verbs :\n",
    "            key_verbs[best_sense]['occorrences'] += 1\n",
    "            key_verbs[best_sense]['rep_event'] += 1 if len(rep_event) > 0 else 0\n",
    "            key_verbs[best_sense]['asp_event'] += 1 if len(asp_event) > 0 else 0 \n",
    "            key_verbs[best_sense]['location'] += 1 if \"GPE\" in entities1 else 0\n",
    "            key_verbs[best_sense]['organization'] += 1 if \"ORG\" in entities2 else 0\n",
    "            if is_verb :\n",
    "                key_verbs[best_sense]['targets'].add(wn_lemmatizer.lemmatize(verb_state, 'v'))\n",
    "            else : \n",
    "                key_verbs[best_sense]['targets'].add(wn_lemmatizer.lemmatize(verb_state))\n",
    "        else :\n",
    "            key_verbs[best_sense] = {}\n",
    "            key_verbs[best_sense]['occorrences'] = 1\n",
    "            key_verbs[best_sense]['definition'] = definition\n",
    "            key_verbs[best_sense]['rep_event'] = 1 if len(rep_event) > 0 else 0\n",
    "            key_verbs[best_sense]['asp_event'] = 1 if len(asp_event) > 0 else 0\n",
    "            key_verbs[best_sense]['location'] = 1 if \"GPE\" in entities1 else 0\n",
    "            key_verbs[best_sense]['organization'] = 1 if \"ORG\" in entities2 else 0\n",
    "            if is_verb :\n",
    "                key_verbs[best_sense]['targets'] = set([wn_lemmatizer.lemmatize(verb_state, 'v')])\n",
    "            else :\n",
    "                key_verbs[best_sense]['targets'] = set([wn_lemmatizer.lemmatize(verb_state)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of annotations (STATE) over 1000 sentences: 677 , sia 37.71587743732591 %.\n",
      "Number of annotations (EVENT) over 1000 sentences: 853 , sia 47.5208913649025 %.\n"
     ]
    }
   ],
   "source": [
    "df_states_quant_analysis = pd.DataFrame(columns=[\n",
    "    'state (WORDNET SENSE IF EXIST)', \n",
    "    'occorrences', \n",
    "    'definition', \n",
    "    'rep_event', \n",
    "    'asp_event', \n",
    "    'location', \n",
    "    'organization', \n",
    "    'targets'\n",
    "])\n",
    "for key, value in verbs['states'].items() :\n",
    "    value_copy = value.copy()\n",
    "    value_copy['state (WORDNET SENSE IF EXIST)'] = key\n",
    "    value_copy['targets'] = \", \".join(value['targets'])\n",
    "    df_states_quant_analysis = df_states_quant_analysis.append(value_copy, ignore_index=True)\n",
    "df_states_quant_analysis = df_states_quant_analysis.sort_values(by='occorrences', ascending=False)\n",
    "print(\n",
    "    \"Number of annotations (STATE) over 1000 sentences:\", \n",
    "    df_states_quant_analysis['occorrences'].sum(),\n",
    "    \", sia\",\n",
    "    (df_states_quant_analysis['occorrences'].sum() * 100) / df_marco.shape[0],\n",
    "    \"%.\"\n",
    ")\n",
    "df_states_quant_analysis.to_csv(states_quantitative_analysis_csv_path, index=False)\n",
    "\n",
    "df_events_quant_analysis = pd.DataFrame(columns=[\n",
    "    'event (WORDNET SENSE IF EXIST)', \n",
    "    'occorrences', \n",
    "    'definition', \n",
    "    'rep_event', \n",
    "    'asp_event', \n",
    "    'location', \n",
    "    'organization', \n",
    "    'targets'\n",
    "])\n",
    "for key, value in verbs['events'].items() :\n",
    "    value_copy = value.copy()\n",
    "    value_copy['event (WORDNET SENSE IF EXIST)'] = key\n",
    "    value_copy['targets'] = \", \".join(value['targets'])\n",
    "    df_events_quant_analysis = df_events_quant_analysis.append(value_copy, ignore_index=True)\n",
    "df_events_quant_analysis = df_events_quant_analysis.sort_values(by='occorrences', ascending=False)\n",
    "print(\n",
    "    \"Number of annotations (EVENT) over 1000 sentences:\", \n",
    "    df_events_quant_analysis['occorrences'].sum(),\n",
    "    \", sia\",\n",
    "    (df_events_quant_analysis['occorrences'].sum() * 100) / df_marco.shape[0],\n",
    "    \"%.\"\n",
    ")\n",
    "df_events_quant_analysis.to_csv(events_quantitative_analysis_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg1 = df_states_quant_analysis.groupby('targets').agg({\n",
    "    'state (WORDNET SENSE IF EXIST)': ' ## '.join, \n",
    "    'occorrences': sum, \n",
    "    'definition': ' ## '.join, \n",
    "    'rep_event': sum, \n",
    "    'asp_event': sum, \n",
    "    'location': sum, \n",
    "    'organization': sum,\n",
    "    'targets': ' ## '.join\n",
    "})\n",
    "df_agg1 = df_agg1.sort_values(by='occorrences', ascending=False)\n",
    "df_agg1.to_csv(states_quantitative_analysis_groupby_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg2 = df_events_quant_analysis.groupby('targets').agg({\n",
    "    'event (WORDNET SENSE IF EXIST)': ' ## '.join, \n",
    "    'occorrences': sum, \n",
    "    'definition': ' ## '.join, \n",
    "    'rep_event': sum, \n",
    "    'asp_event': sum, \n",
    "    'location': sum, \n",
    "    'organization': sum,\n",
    "    'targets': ' ## '.join\n",
    "})\n",
    "df_agg2 = df_agg2.sort_values(by='occorrences', ascending=False)\n",
    "df_agg2.to_csv(events_quantitative_analysis_groupby_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('receive.v.02') receive a specified treatment (abstract)\n",
      "Synset('receive.v.02') receive a specified treatment (abstract)\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Ramachandran, whose father wanted him to become a physician rather than a researcher, obtained an M.B.B.S. from Stanley Medical College in Chennai, India.\"\n",
    "sense = lesk(word_tokenize(sentence), 'obtained', pos=wn.VERB)\n",
    "print(sense, sense.definition())\n",
    "\n",
    "sentence = \"He received the Kerala Sahitya Academi award in the `poetry' section for his collection, Nellickal Muraleedharante Kavithakal in 2004.\"\n",
    "sense = lesk(word_tokenize(sentence), 'obtained', pos=wn.VERB)\n",
    "print(sense, sense.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "Synset('go.n.01') a time for working (after which you will be relieved by someone else)\n",
      " => it's my go\n",
      " => a spell of work\n",
      "Synset('adam.n.03') street names for methylenedioxymethamphetamine\n",
      "Synset('crack.n.09') a usually brief attempt\n",
      " => he took a crack at it\n",
      " => I gave it a whirl\n",
      "Synset('go.n.04') a board game for two players who place counters on a grid; the object is to surround and so capture the opponent's counters\n",
      "Synset('travel.v.01') change location; move, travel, or proceed, also metaphorically\n",
      " => How fast does your new car go?\n",
      " => We travelled from Rome to Naples by bus\n",
      " => The policemen went from door to door looking for the suspect\n",
      " => The soldiers moved towards the city in an attempt to take it before night fell\n",
      " => news travelled fast\n",
      "Synset('go.v.02') follow a procedure or take a course\n",
      " => We should go farther in this matter\n",
      " => She went through a lot of trouble\n",
      " => go about the world in a certain manner\n",
      " => Messages must go through diplomatic channels\n",
      "Synset('go.v.03') move away from a place into another direction\n",
      " => Go away before I start to cry\n",
      " => The train departs at noon\n",
      "Synset('become.v.01') enter or assume a certain state or condition\n",
      " => He became annoyed when he heard the bad news\n",
      " => It must be getting more serious\n",
      " => her face went red with anger\n",
      " => She went into ecstasy\n",
      " => Get going!\n",
      "Synset('go.v.05') be awarded; be allotted\n",
      " => The first prize goes to Mary\n",
      " => Her money went on clothes\n",
      "Synset('run.v.05') have a particular form\n",
      " => the story or argument runs as follows\n",
      " => as the saying goes...\n",
      "Synset('run.v.03') stretch out over a distance, space, time, or scope; run or extend between two points or beyond a certain point\n",
      " => Service runs all the way to Cranbury\n",
      " => His knowledge doesn't go very far\n",
      " => My memory extends back to my fourth year of life\n",
      " => The facts extend beyond a consideration of her personal assets\n",
      "Synset('proceed.v.04') follow a certain course\n",
      " => The inauguration went well\n",
      " => how did your interview go?\n",
      "Synset('go.v.09') be abolished or discarded\n",
      " => These ugly billboards have to go!\n",
      " => These luxuries all had to go under the Khmer Rouge\n",
      "Synset('go.v.10') be or continue to be in a certain condition\n",
      " => The children went hungry that day\n",
      "Synset('sound.v.02') make a certain noise or sound\n",
      " => She went `Mmmmm'\n",
      " => The gun went `bang'\n",
      "Synset('function.v.01') perform as expected when applied\n",
      " => The washing machine won't go unless it's plugged in\n",
      " => Does this old car still run well?\n",
      " => This old radio doesn't work anymore\n",
      "Synset('run_low.v.01') to be spent or finished\n",
      " => The money had gone after a few days\n",
      " => Gas is running low at the gas stations in the Midwest\n",
      "Synset('move.v.13') progress by being changed\n",
      " => The speech has to go through several more drafts\n",
      " => run through your presentation before the meeting\n",
      "Synset('survive.v.01') continue to live through hardship or adversity\n",
      " => We went without water and food for 3 days\n",
      " => These superstitions survive in the backwaters of America\n",
      " => The race car driver lived through several very serious accidents\n",
      " => how long can a person last without food and water?\n",
      "Synset('go.v.16') pass, fare, or elapse; of a certain state of affairs or action\n",
      " => How is it going?\n",
      " => The day went well until I got your call\n",
      "Synset('die.v.01') pass from physical life and lose all bodily attributes and functions necessary to sustain life\n",
      " => She died from cancer\n",
      " => The children perished in the fire\n",
      " => The patient went peacefully\n",
      " => The old guy kicked the bucket at the age of 102\n",
      "Synset('belong.v.03') be in the right place or situation\n",
      " => Where do these books belong?\n",
      " => Let's put health care where it belongs--under the control of the government\n",
      " => Where do these books go?\n",
      "Synset('go.v.19') be ranked or compare\n",
      " => This violinist is as good as Juilliard-trained violinists go\n",
      "Synset('start.v.09') begin or set in motion\n",
      " => I start at eight in the morning\n",
      " => Ready, set, go!\n",
      "Synset('move.v.15') have a turn; make one's move in a game\n",
      " => Can I go now?\n",
      "Synset('go.v.22') be contained in\n",
      " => How many times does 18 go into 54?\n",
      "Synset('go.v.23') be sounded, played, or expressed\n",
      " => How does this song go again?\n",
      "Synset('blend.v.02') blend or harmonize\n",
      " => This flavor will blend with those in your dish\n",
      " => This sofa won't go with the chairs\n",
      "Synset('go.v.25') lead, extend, or afford access\n",
      " => This door goes to the basement\n",
      " => The road runs South\n",
      "Synset('fit.v.02') be the right size or shape; fit correctly or as desired\n",
      " => This piece won't fit into the puzzle\n",
      "Synset('rifle.v.02') go through in search of something; search through someone's belongings in an unauthorized way\n",
      " => Who rifled through my desk drawers?\n",
      "Synset('go.v.28') be spent\n",
      " => All my money went for food and rent\n",
      "Synset('plump.v.04') give support (to) or make a choice (of) one out of a group or number\n",
      " => I plumped for the losing candidates\n",
      "Synset('fail.v.04') stop operating or functioning\n",
      " => The engine finally went\n",
      " => The car died on the road\n",
      " => The bus we travelled in broke down on the way to town\n",
      " => The coffee maker broke\n",
      " => The engine failed on the way to town\n",
      " => her eyesight went after the accident\n",
      "Synset('go.a.01') functioning correctly and ready for action\n",
      " => all systems are go\n"
     ]
    }
   ],
   "source": [
    "senses = wn.synsets('go')\n",
    "print(len(senses))\n",
    "for sense in senses :\n",
    "    print(sense, sense.definition())\n",
    "    for example in sense.examples() :\n",
    "        print(\" =>\", example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "print(len(senses))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9c0098ef03c01875785b14fb83d5161d4a7df326e3b00d12d4d2605823f1c481"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
