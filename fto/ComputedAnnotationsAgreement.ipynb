{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from stop_words import get_stop_words\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Customer Variables\n",
    "annotations_ousmane_200_json_path = \"inputs\\\\annotations-ousmane-200.json\" \n",
    "annotations_rossana_200_json_path = \"inputs\\\\rossana_annotations.json\" \n",
    "annotations_enrico_100_json_path = \"inputs\\\\enrico_annotations.json\" \n",
    "annotations_ousmane_200_csv_path = \"inputs\\\\annotations-ousmane-200.csv\"\n",
    "annotations_marco_1000_csv_path = \"inputs\\\\1000annotazioni.csv\"\n",
    "annotations_marco_200_csv_path = \"inputs\\\\annotations-marco-200.csv\"\n",
    "annotations_marco_200_2_csv_path = \"inputs\\\\annotations-marco-200-2.csv\"\n",
    "annotations_marco_100_csv_path = \"inputs\\\\annotations-marco-100.csv\"\n",
    "annotations_rossana_200_csv_path = \"inputs\\\\annotations-rossana-200.csv\"\n",
    "annotations_enrico_100_csv_path = \"inputs\\\\annotations-enrico-100.csv\"\n",
    "annotations_matched_200_csv_path = \"outputs\\\\annotations-matched-200.csv\"\n",
    "annotations_contains_matched_200_csv_path = \"outputs\\\\annotations_contains-matched-200.csv\"\n",
    "states_quantitative_analysis_csv_path = \"outputs\\\\states_quantitative_analysis.csv\"\n",
    "events_quantitative_analysis_csv_path = \"outputs\\\\events_quantitative_analysis.csv\"\n",
    "states_quantitative_analysis_groupby_lemmas = \"outputs\\\\states_quantitative_analysis_groupby_lemmas.csv\"\n",
    "events_quantitative_analysis_groupby_lemmas = \"outputs\\\\events_quantitative_analysis_groupby_lemmas.csv\"\n",
    "test_csv_path = \"outputs\\\\test.csv\"\n",
    "temp_computing_path = \"outputs\\\\temp_computing.csv\"\n",
    "exact_agreement_columns = [\"author\", \"sent_id\", \"text\", \"TIME\", \"WRITER-AG\", \"EVENT\", \"ORG\", \"LOC\", \"ASP-EVENT\", \"STATE\", \"WRITER-PA\", \"REP-EVENT\"]\n",
    "soft_agreement_columns = [\"author\", \"sent_id\", \"text\", \"EVENT\", \"STATE\"]\n",
    "merge_by_columns = [\"author\", \"sent_id\", \"text\"]\n",
    "element_types_without_events_and_states = [\"WRITER-AG\", \"ORG\", \"LOC\", \"ASP-EVENT\", \"WRITER-PA\", \"REP-EVENT\"]\n",
    "element_types = [\"TIME\", \"WRITER-AG\", \"EVENT\", \"ORG\", \"LOC\", \"ASP-EVENT\", \"STATE\", \"WRITER-PA\", \"REP-EVENT\"]\n",
    "\n",
    "## Libraries Variables\n",
    "stop_words = list(get_stop_words('en'))         #About 900 stopwords\n",
    "nltk_words = list(stopwords.words('english')) #About 150 stopwords\n",
    "stop_words.extend(nltk_words)\n",
    "wn_lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function convert the annotation file json in a annotation file csv.\n",
    "def annotations_json_to_csv(path_file_json, annotator_name, path_file_csv) :\n",
    "    df = pd.DataFrame()\n",
    "    with open(path_file_json, \"r\", encoding='utf-8') as f:\n",
    "        file_contents = json.load(f)\n",
    "        for item in file_contents:\n",
    "            for annotation in item['annotations']:\n",
    "                container = dict()\n",
    "                container['annotator'] = annotator_name\n",
    "                container['author'] = item['data']['author']\n",
    "                container['sent_id'] = item['data']['sent_id']\n",
    "                container['text'] = item['data']['text']\n",
    "                for v in annotation['result']:\n",
    "                    container[v['value']['labels'][0]] = v['value']['text']\n",
    "                df = df.append(container, ignore_index=True)\n",
    "    df.to_csv(path_file_csv, index=False)\n",
    "\n",
    "# This function verify if string elements of two list are \"contains relations\".\n",
    "# \"contains relations\": for example we have two string str1 and str2, if str1 is sub strinf of str2\n",
    "# or str2 is sub string of str1\n",
    "def checkContainsRelations(items_1, items_2) :\n",
    "    computed = True\n",
    "    if len(items_1) != len(items_2) : return False\n",
    "    for index in range(len(items_1)) : \n",
    "        #computed = computed and (str(items_1[index]) in str(items_2[index]) or str(items_2[index]) in str(items_1[index]))\n",
    "        sub_compute = True\n",
    "        lx = str(items_1[index]).split()\n",
    "        ly = str(items_2[index]).split()\n",
    "        if len(lx) > len(ly) :\n",
    "            lx, ly = swapedItems(lx, ly)\n",
    "        for i in range(len(lx)) :\n",
    "            sub_compute = sub_compute and (lx[i] in ly)\n",
    "            if not sub_compute :\n",
    "                break\n",
    "        computed = computed and sub_compute\n",
    "        if not computed : return False\n",
    "    return computed\n",
    "\n",
    "# Contain relation between two string\n",
    "def checkContainRelation(item_1, item_2) :\n",
    "    computed = True\n",
    "    lx = str(item_1).split()\n",
    "    ly = str(item_2).split()\n",
    "    if len(lx) > len(ly) :\n",
    "        lx, ly = swapedItems(lx, ly)\n",
    "    for i in range(len(lx)) :\n",
    "        computed = computed and (lx[i] in ly)\n",
    "        if not computed :\n",
    "            return False\n",
    "    return computed\n",
    "\n",
    "\n",
    "## This function merge two csv with Pandas\n",
    "def merged_two_csv(path_csv1, path_csv2, how_mode, onColumns) :\n",
    "    df1 = pd.read_csv(path_csv1)\n",
    "    df2 = pd.read_csv(path_csv2)\n",
    "    df = pd.merge(df1, df2, how=how_mode, on=onColumns) \n",
    "    return df\n",
    "\n",
    "def concateItems(item_1, item_2) :\n",
    "    item = \"Value1: \"\n",
    "    item += str(item_1) if str(item_1) != \"nan\" else \"MISSING_VALUE\"\n",
    "    item += \" & Value2: \"\n",
    "    item += str(item_2) if str(item_2) != \"nan\" else \"MISSING_VALUE\"\n",
    "    return item\n",
    "\n",
    "def swapedItems(item_1, item_2) :\n",
    "    temp = item_1\n",
    "    item_1 = item_2\n",
    "    item_2 = temp\n",
    "    return item_1,item_2\n",
    "\n",
    "## \n",
    "def computeContainsRelationMatchByEventOrState(df_annotations1, df_annotations2, output_file_name=\"\") :\n",
    "    df_set_pivot = df_annotations1[[\"author\", \"sent_id\", \"text\"]].drop_duplicates(subset=[\"author\", \"sent_id\", \"text\"])\n",
    "\n",
    "    # Create a matched Dictionnary container\n",
    "    matched_container = {\n",
    "        'Author': [],\n",
    "        'SentenceId': [],\n",
    "        'EVENT': [],\n",
    "        'STATE': [],\n",
    "        'MatchedInfo': [],\n",
    "        'Text': []\n",
    "    }\n",
    "\n",
    "    # Iterate Pivot df and Processing soft matching on Some columns \n",
    "    for _, row in df_set_pivot.iterrows():\n",
    "        author = row[\"author\"]\n",
    "        sent_id = row[\"sent_id\"]\n",
    "        text = row[\"text\"]\n",
    "        df_x = df_annotations1.loc[(df_annotations1['author'] == author) & (df_annotations1['sent_id'] == sent_id)]\n",
    "        df_y = df_annotations2.loc[(df_annotations2['author'] == author) & (df_annotations2['sent_id'] == sent_id)]\n",
    "        if (df_x.shape[0] < df_y.shape[0]) :\n",
    "            df_x, df_y = swapedItems(df_x, df_y)\n",
    "        for _, row_x in df_x.iterrows() :\n",
    "            items_1 = [row_x[\"EVENT\"], row_x[\"STATE\"]]\n",
    "            for index_y, row_y in df_y.iterrows() :\n",
    "                items_2 = [row_y[\"EVENT\"], row_y[\"STATE\"]]\n",
    "                if items_1 == items_2 : \n",
    "                    matched_container['Author'].append(author)\n",
    "                    matched_container['SentenceId'].append(sent_id)\n",
    "                    matched_container['Text'].append(text)\n",
    "                    matched_container['EVENT'].append(row_x[\"EVENT\"])\n",
    "                    matched_container['STATE'].append(row_x[\"STATE\"])\n",
    "                    matched_container['MatchedInfo'].append(\"Exact Match\")\n",
    "                    df_y = df_y.drop(index_y)\n",
    "                    break\n",
    "                elif checkContainsRelations(items_1, items_2) :\n",
    "                    matched_container['Author'].append(author)\n",
    "                    matched_container['SentenceId'].append(sent_id)\n",
    "                    matched_container['Text'].append(text)\n",
    "                    if str(row_x[\"EVENT\"]) != \"nan\" or str(row_y[\"EVENT\"]) != \"nan\" : \n",
    "                        matched_container['EVENT'].append(concateItems(row_x[\"EVENT\"],row_y[\"EVENT\"]))\n",
    "                    else : matched_container['EVENT'].append(\"\")\n",
    "                    if str(row_x[\"STATE\"]) != \"nan\" or str(row_y[\"STATE\"]) != \"nan\" : \n",
    "                        matched_container['STATE'].append(concateItems(row_x[\"STATE\"], row_y[\"STATE\"]))\n",
    "                    else : matched_container['STATE'].append(\"\")\n",
    "                    matched_container['MatchedInfo'].append(\"Contains Relation Match\")\n",
    "                    df_y = df_y.drop(index_y)\n",
    "                    break\n",
    "\n",
    "    # Convert the dictionary into DataFrame\n",
    "    df_contains_matched = pd.DataFrame(matched_container, columns = ['EVENT', 'STATE', 'MatchedInfo', 'SentenceId', 'Text'])\n",
    "    df_contains_matched.replace(np.nan, '', regex=True)\n",
    "    if output_file_name != \"\" and isinstance(output_file_name, str) : \n",
    "        df_contains_matched.to_csv(output_file_name, index=False)\n",
    "    return df_contains_matched\n",
    "\n",
    "## This function take two annotations Dataframe and compute the Agreement by Category\n",
    "def computeContainsRelationMatchByElementType(df_annotations1, df_annotations2, element_type, output_file_name) : \n",
    "    \n",
    "    # Create a matched Dictionnary container\n",
    "    matched_container = {\n",
    "        'Author': [],\n",
    "        'SentenceId': [],\n",
    "        element_type: [],\n",
    "        'MatchedInfo': [],\n",
    "        'Text': []\n",
    "    }\n",
    "\n",
    "    if element_type not in element_types_without_events_and_states :\n",
    "        raise Exception(\"Element Type not exist or has type EVENT or STATE!\")\n",
    "\n",
    "    # Compute Pivot Set\n",
    "    df_set_pivot = df_annotations1[[\"author\", \"sent_id\", \"text\"]].drop_duplicates(subset=[\"author\", \"sent_id\", \"text\"])\n",
    "\n",
    "    for _, row in df_set_pivot.iterrows() :\n",
    "        author = row[\"author\"]\n",
    "        sent_id = row[\"sent_id\"]\n",
    "        text = row[\"text\"]\n",
    "        df_x = df_annotations1.loc[(df_annotations1['author'] == author) & (df_annotations1['sent_id'] == sent_id) & (df_annotations1['text'] == text)]\n",
    "        df_y = df_annotations2.loc[(df_annotations2['author'] == author) & (df_annotations2['sent_id'] == sent_id) & (df_annotations2['text'] == text)]\n",
    "        if (df_x.shape[0] < df_y.shape[0]) :\n",
    "            df_x, df_y = swapedItems(df_x, df_y)\n",
    "        for _, row_x in df_x.iterrows() :\n",
    "            items_1 = [row_x[element_type]]\n",
    "            for index_y, row_y in df_y.iterrows() :\n",
    "                items_2 = [row_y[element_type]]\n",
    "                if items_1 == items_2 : \n",
    "                    matched_container['Author'].append(author)\n",
    "                    matched_container['SentenceId'].append(sent_id)\n",
    "                    matched_container['Text'].append(text)\n",
    "                    matched_container[element_type].append(row_x[element_type])\n",
    "                    matched_container['MatchedInfo'].append(\"Exact Match\")\n",
    "                    df_y = df_y.drop(index_y)\n",
    "                    break\n",
    "                elif checkContainsRelations(items_1, items_2) :\n",
    "                    matched_container['Author'].append(author)\n",
    "                    matched_container['SentenceId'].append(sent_id)\n",
    "                    matched_container['Text'].append(text)\n",
    "                    if str(row_x[element_type]) != \"nan\" or str(row_y[element_type]) != \"nan\" : \n",
    "                        matched_container[element_type].append(concateItems(row_x[element_type],row_y[element_type]))\n",
    "                    else : matched_container[element_type].append(\"\")\n",
    "                    matched_container['MatchedInfo'].append(\"Contains Relation Match\")\n",
    "                    df_y = df_y.drop(index_y)\n",
    "                    break\n",
    "        \n",
    "    # Convert the dictionary into DataFrame\n",
    "    df_contains_matched = pd.DataFrame(matched_container, columns = [element_type, 'MatchedInfo', 'SentenceId', 'Text'])\n",
    "    df_contains_matched.replace(np.nan, '', regex=True)\n",
    "    df_contains_matched.to_csv(output_file_name, index=False)\n",
    "    return df_contains_matched\n",
    "\n",
    "# Precision e Recall X Categoria, First Annotations will be considerate GOLD.\n",
    "def precisionAndRecallXCategory(df_annotations1, df_annotations2, category) :\n",
    "\n",
    "    a_1 = df_annotations1.dropna(subset=[category]).drop_duplicates(subset=['author','sent_id',category])\n",
    "    a_2 = df_annotations2.dropna(subset=[category]).drop_duplicates(subset=['author','sent_id',category])\n",
    "\n",
    "    merged = a_1[['author','sent_id',category]].merge(a_2[['author','sent_id',category]],on=['author','sent_id'])\n",
    "\n",
    "    a_1 = merged[['author','sent_id','{}_x'.format(category)]].dropna(subset=['{}_x'.format(category)]).drop_duplicates(subset=['author','sent_id','{}_x'.format(category)]).reset_index().drop(columns='index')\n",
    "    a_2 = merged[['author','sent_id','{}_y'.format(category)]].dropna(subset=['{}_y'.format(category)]).drop_duplicates(subset=['author','sent_id','{}_y'.format(category)]).reset_index().drop(columns='index')\n",
    "    \n",
    "    merged = a_1.merge(a_2)\n",
    "    merged = merged.groupby(['author','sent_id','{}_x'.format(category)])['{}_y'.format(category)].apply(list)\n",
    "    merged = merged.reset_index()\n",
    "\n",
    "    true_pos = 0\n",
    "    false_pos = 0\n",
    "    \n",
    "    for row in merged.iloc[:].values:\n",
    "        agreement = False\n",
    "        for el in row[3]:\n",
    "            searched = re.search(row[2],el)\n",
    "            if searched:\n",
    "                agreement = True\n",
    "            else: continue\n",
    "        if agreement is True:\n",
    "            true_pos +=1\n",
    "        else: false_pos+=1\n",
    "    return {'true_pos': true_pos, 'false_pos': false_pos, 'retrieved': len(a_1), 'relevant': len(a_2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trnasform Annotions file JSON to file CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#annotations_json_to_csv(annotations_enrico_100_json_path, \"enrico\", annotations_enrico_100_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute exact Agreement \n",
    "### Output file => \"outputs\\\\annotations-matched-200.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exact_matched = merged_two_csv(annotations_ousmane_200_csv_path, annotations_marco_1000_csv_path, \"inner\", exact_agreement_columns)\n",
    "df_exact_matched.to_csv(annotations_matched_200_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute soft Agreement (columns values are contains relations)\n",
    "### Output file => \"outputs\\\\annotations_contains-matched-200.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Both df of annotatore and  make Pivot df\n",
    "df_ousmane = pd.read_csv(annotations_ousmane_200_csv_path)\n",
    "df_marco_200 = pd.read_csv(annotations_marco_200_csv_path)\n",
    "df_contains_matched = computeContainsRelationMatchByEventOrState(df_ousmane, df_marco_200, annotations_contains_matched_200_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+-------------------------+----------------------+---------------------+-------------+-------------------------+----------------+--------------------+\n",
      "|        GOLD ANNOTATIONS        |    Intersection Mode    | Precision Evaluation |  Recall Evaluation  | Exact Match | Contains Relation Match |  Type Element  |      F-score       |\n",
      "+--------------------------------+-------------------------+----------------------+---------------------+-------------+-------------------------+----------------+--------------------+\n",
      "|  Annoations of Fist Annotator  |       Exact Match       |  0.3333333333333333  | 0.27823691460055094 |     101     |            0            |  ALL ELEMENT   | 0.3033033033033033 |\n",
      "|  Annoations of Fist Annotator  | Contains Relation Match |  0.6996699669966997  |  0.5840220385674931 |     205     |            7            | EVENT or STATE | 0.6366366366366366 |\n",
      "| Annoations of Second Annotator |       Exact Match       | 0.27823691460055094  |  0.3333333333333333 |     101     |            0            |  ALL ELEMENT   | 0.3033033033033033 |\n",
      "| Annoations of Second Annotator | Contains Relation Match |  0.5840220385674931  |  0.6996699669966997 |     205     |            7            | EVENT or STATE | 0.6366366366366366 |\n",
      "+--------------------------------+-------------------------+----------------------+---------------------+-------------+-------------------------+----------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t = PrettyTable(['GOLD ANNOTATIONS', 'Intersection Mode', 'Precision Evaluation', 'Recall Evaluation', 'Exact Match', 'Contains Relation Match', 'Type Element', 'F-score'])\n",
    "\n",
    "exaxt_match = df_contains_matched[df_contains_matched.MatchedInfo == 'Exact Match'].shape[0]\n",
    "contains_relation_match = df_contains_matched[df_contains_matched.MatchedInfo == 'Contains Relation Match'].shape[0]\n",
    "\n",
    "precision = (df_exact_matched.shape[0])/(df_ousmane.shape[0])\n",
    "recall = (df_exact_matched.shape[0])/(df_marco_200.shape[0])\n",
    "f_score = (2*precision*recall)/(precision+recall)\n",
    "t.add_row(['Annoations of Fist Annotator', 'Exact Match', precision, recall, df_exact_matched.shape[0], 0, 'ALL ELEMENT', f_score])\n",
    "\n",
    "precision = (df_contains_matched.shape[0])/(df_ousmane.shape[0])\n",
    "recall = (df_contains_matched.shape[0])/(df_marco_200.shape[0])\n",
    "f_score = (2*precision*recall)/(precision+recall)\n",
    "t.add_row(['Annoations of Fist Annotator', 'Contains Relation Match', precision, recall, exaxt_match, contains_relation_match, 'EVENT or STATE', f_score])\n",
    "\n",
    "precision = (df_exact_matched.shape[0])/(df_marco_200.shape[0])\n",
    "recall = (df_exact_matched.shape[0])/(df_ousmane.shape[0])\n",
    "f_score = (2*precision*recall)/(precision+recall)\n",
    "t.add_row(['Annoations of Second Annotator', 'Exact Match', precision, recall, df_exact_matched.shape[0], 0, 'ALL ELEMENT', f_score])\n",
    "\n",
    "precision = (df_contains_matched.shape[0])/(df_marco_200.shape[0])\n",
    "recall = (df_contains_matched.shape[0])/(df_ousmane.shape[0])\n",
    "f_score = (2*precision*recall)/(precision+recall)\n",
    "t.add_row(['Annoations of Second Annotator', 'Contains Relation Match', precision, recall, exaxt_match, contains_relation_match, 'EVENT or STATE', f_score])\n",
    "\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+-------------------------+----------------------+--------------------+-------------+-------------------------+--------------+\n",
      "|        GOLD ANNOTATIONS        |    Intersection Mode    | Precision Evaluation | Recall Evaluation  | Exact Match | Contains Relation Match | Type Element |\n",
      "+--------------------------------+-------------------------+----------------------+--------------------+-------------+-------------------------+--------------+\n",
      "|  Annoations of Fist Annotator  | Contains Relation Match |  0.7162534435261708  | 0.858085808580858  |     257     |            3            |  WRITER-AG   |\n",
      "| Annoations of Second Annotator | Contains Relation Match |  0.858085808580858   | 0.7162534435261708 |     257     |            3            |  WRITER-AG   |\n",
      "|                                |                         |                      |                    |             |                         |              |\n",
      "|  Annoations of Fist Annotator  | Contains Relation Match |  0.6611570247933884  | 0.7920792079207921 |     220     |            20           |     ORG      |\n",
      "| Annoations of Second Annotator | Contains Relation Match |  0.7920792079207921  | 0.6611570247933884 |     220     |            20           |     ORG      |\n",
      "|                                |                         |                      |                    |             |                         |              |\n",
      "|  Annoations of Fist Annotator  | Contains Relation Match |  0.7024793388429752  | 0.8415841584158416 |     241     |            14           |     LOC      |\n",
      "| Annoations of Second Annotator | Contains Relation Match |  0.8415841584158416  | 0.7024793388429752 |     241     |            14           |     LOC      |\n",
      "|                                |                         |                      |                    |             |                         |              |\n",
      "|  Annoations of Fist Annotator  | Contains Relation Match |  0.7658402203856749  | 0.9174917491749175 |     278     |            0            |  ASP-EVENT   |\n",
      "| Annoations of Second Annotator | Contains Relation Match |  0.9174917491749175  | 0.7658402203856749 |     278     |            0            |  ASP-EVENT   |\n",
      "|                                |                         |                      |                    |             |                         |              |\n",
      "|  Annoations of Fist Annotator  | Contains Relation Match |  0.7493112947658402  | 0.8976897689768977 |     270     |            2            |  WRITER-PA   |\n",
      "| Annoations of Second Annotator | Contains Relation Match |  0.8976897689768977  | 0.7493112947658402 |     270     |            2            |  WRITER-PA   |\n",
      "|                                |                         |                      |                    |             |                         |              |\n",
      "|  Annoations of Fist Annotator  | Contains Relation Match |  0.790633608815427   | 0.9471947194719472 |     287     |            0            |  REP-EVENT   |\n",
      "| Annoations of Second Annotator | Contains Relation Match |  0.9471947194719472  | 0.790633608815427  |     287     |            0            |  REP-EVENT   |\n",
      "|                                |                         |                      |                    |             |                         |              |\n",
      "+--------------------------------+-------------------------+----------------------+--------------------+-------------+-------------------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "t = PrettyTable(['GOLD ANNOTATIONS', 'Intersection Mode', 'Precision Evaluation', 'Recall Evaluation', 'Exact Match', 'Contains Relation Match', 'Type Element'])\n",
    "for type_element in element_types_without_events_and_states :\n",
    "    df_temp = computeContainsRelationMatchByElementType(df_ousmane, df_marco_200, type_element, temp_computing_path)\n",
    "\n",
    "    exaxt_match = df_temp[df_temp.MatchedInfo == 'Exact Match'].shape[0]\n",
    "    contains_relation_match = df_temp[df_temp.MatchedInfo == 'Contains Relation Match'].shape[0]\n",
    "\n",
    "    precision = (df_temp.shape[0])/(df_marco_200.shape[0])\n",
    "    recall = (df_temp.shape[0])/(df_ousmane.shape[0])\n",
    "    t.add_row(['Annoations of Fist Annotator', 'Contains Relation Match', precision, recall, exaxt_match, contains_relation_match, type_element])\n",
    "\n",
    "    precision = (df_temp.shape[0])/(df_ousmane.shape[0])\n",
    "    recall = (df_temp.shape[0])/(df_marco_200.shape[0])\n",
    "    t.add_row(['Annoations of Second Annotator', 'Contains Relation Match', precision, recall, exaxt_match, contains_relation_match, type_element])\n",
    "    \n",
    "    t.add_row(['', '', '', '', '', '', ''])\n",
    "\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------------------------+----------------------+---------------------+-------------+-------------------------+----------------+--------------------+\n",
      "|        GOLD ANNOTATIONS       |    Intersection Mode    | Precision Evaluation |  Recall Evaluation  | Exact Match | Contains Relation Match |  Type Element  |      F-score       |\n",
      "+-------------------------------+-------------------------+----------------------+---------------------+-------------+-------------------------+----------------+--------------------+\n",
      "|  Annoations of Fist Annotator |       Exact Match       |  0.2865853658536585  | 0.27485380116959063 |      94     |            0            |  ALL ELEMENT   | 0.2805970149253731 |\n",
      "|  Annoations of Fist Annotator | Contains Relation Match |  0.6676829268292683  |  0.6403508771929824 |     213     |            6            | EVENT or STATE | 0.6537313432835822 |\n",
      "| Annoations of third annotator |       Exact Match       | 0.27485380116959063  |  0.2865853658536585 |      94     |            0            |  ALL ELEMENT   | 0.2805970149253731 |\n",
      "| Annoations of third annotator | Contains Relation Match |  0.6403508771929824  |  0.6676829268292683 |     213     |            6            | EVENT or STATE | 0.6537313432835822 |\n",
      "+-------------------------------+-------------------------+----------------------+---------------------+-------------+-------------------------+----------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "df_marco = pd.read_csv(annotations_marco_1000_csv_path).replace(np.nan, '', regex=True)\n",
    "df_rossana = pd.read_csv(annotations_rossana_200_csv_path)\n",
    "df_marco_200_second = pd.read_csv(annotations_marco_200_2_csv_path)\n",
    "\n",
    "#df_marco_200_second = pd.DataFrame(columns=list(df_rossana.columns))\n",
    "#df_set_pivot = df_rossana[[\"author\", \"sent_id\", \"text\"]].drop_duplicates(subset=[\"author\", \"sent_id\", \"text\"])\n",
    "#for _, row in df_set_pivot.iterrows() :\n",
    "#    author = row[\"author\"]\n",
    "#    sent_id = row[\"sent_id\"]\n",
    "#    text = row[\"text\"]\n",
    "#    df_ = df_marco.loc[(df_marco['author'] == author) & (df_marco['sent_id'] == sent_id) & (df_marco['text'] == text)]\n",
    "#    for _, row_ in df_.iterrows() :\n",
    "#        df_marco_200_second = df_marco_200_second.append(row_)\n",
    "#df_marco_200_second.to_csv(annotations_marco_200_2_csv_path)\n",
    "\n",
    "t = PrettyTable(['GOLD ANNOTATIONS', 'Intersection Mode', 'Precision Evaluation', 'Recall Evaluation', 'Exact Match', 'Contains Relation Match', 'Type Element', 'F-score'])\n",
    "\n",
    "df_exact_matched = merged_two_csv(annotations_marco_200_2_csv_path, annotations_rossana_200_csv_path, \"inner\", exact_agreement_columns)\n",
    "df_contains_matched = computeContainsRelationMatchByEventOrState(df_marco_200_second, df_rossana)\n",
    "\n",
    "exaxt_match = df_contains_matched[df_contains_matched.MatchedInfo == 'Exact Match'].shape[0]\n",
    "contains_relation_match = df_contains_matched[df_contains_matched.MatchedInfo == 'Contains Relation Match'].shape[0]\n",
    "\n",
    "precision = (df_exact_matched.shape[0])/(df_rossana.shape[0])\n",
    "recall = (df_exact_matched.shape[0])/(df_marco_200_second.shape[0])\n",
    "f_score = (2*precision*recall)/(precision+recall)\n",
    "t.add_row(['Annoations of Fist Annotator', 'Exact Match', precision, recall, df_exact_matched.shape[0], 0, 'ALL ELEMENT', f_score])\n",
    "\n",
    "precision = (df_contains_matched.shape[0])/(df_rossana.shape[0])\n",
    "recall = (df_contains_matched.shape[0])/(df_marco_200_second.shape[0])\n",
    "f_score = (2*precision*recall)/(precision+recall)\n",
    "t.add_row(['Annoations of Fist Annotator', 'Contains Relation Match', precision, recall, exaxt_match, contains_relation_match, 'EVENT or STATE', f_score])\n",
    "\n",
    "precision = (df_exact_matched.shape[0])/(df_marco_200_second.shape[0])\n",
    "recall = (df_exact_matched.shape[0])/(df_rossana.shape[0])\n",
    "f_score = (2*precision*recall)/(precision+recall)\n",
    "t.add_row(['Annoations of third annotator', 'Exact Match', precision, recall, df_exact_matched.shape[0], 0, 'ALL ELEMENT', f_score])\n",
    "\n",
    "precision = (df_contains_matched.shape[0])/(df_marco_200_second.shape[0])\n",
    "recall = (df_contains_matched.shape[0])/(df_rossana.shape[0])\n",
    "f_score = (2*precision*recall)/(precision+recall)\n",
    "t.add_row(['Annoations of third annotator', 'Contains Relation Match', precision, recall, exaxt_match, contains_relation_match, 'EVENT or STATE', f_score])\n",
    "\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+-------------------------+----------------------+--------------------+-------------+-------------------------+----------------+--------------------+\n",
      "|        GOLD ANNOTATIONS        |    Intersection Mode    | Precision Evaluation | Recall Evaluation  | Exact Match | Contains Relation Match |  Type Element  |      F-score       |\n",
      "+--------------------------------+-------------------------+----------------------+--------------------+-------------+-------------------------+----------------+--------------------+\n",
      "|  Annoations of Fist Annotator  | Contains Relation Match |  0.9440559440559441  | 0.7258064516129032 |      96     |            39           | EVENT or STATE | 0.8206686930091186 |\n",
      "| Annoations of fourth annotator | Contains Relation Match |  0.7258064516129032  | 0.9440559440559441 |      96     |            39           | EVENT or STATE | 0.8206686930091186 |\n",
      "+--------------------------------+-------------------------+----------------------+--------------------+-------------+-------------------------+----------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "df_marco = pd.read_csv(annotations_marco_1000_csv_path).replace(np.nan, '', regex=True)\n",
    "df_enrico = pd.read_csv(annotations_enrico_100_csv_path).replace(np.nan, '', regex=True)\n",
    "df_marco_100 = pd.read_csv(annotations_marco_100_csv_path).replace(np.nan, '', regex=True)\n",
    "\n",
    "#df_marco_100 = pd.DataFrame(columns=list(df_enrico.columns))\n",
    "#df_set_pivot = df_enrico[[\"author\", \"sent_id\", \"text\"]].drop_duplicates(subset=[\"author\", \"sent_id\", \"text\"])\n",
    "#for _, row in df_set_pivot.iterrows() :\n",
    "#    author = row[\"author\"]\n",
    "#    sent_id = row[\"sent_id\"]\n",
    "#    text = row[\"text\"]\n",
    "#    df_ = df_marco.loc[(df_marco['author'] == author) & (df_marco['sent_id'] == sent_id)]\n",
    "#    for _, row_ in df_.iterrows() :\n",
    "#        df_marco_100 = df_marco_100.append(row_)\n",
    "\n",
    "t = PrettyTable(['GOLD ANNOTATIONS', 'Intersection Mode', 'Precision Evaluation', 'Recall Evaluation', 'Exact Match', 'Contains Relation Match', 'Type Element', 'F-score'])\n",
    "\n",
    "merge_on_columns = exact_agreement_columns.copy()\n",
    "merge_on_columns.remove(\"ASP-EVENT\")\n",
    "df_exact_matched = df_marco_100.merge(df_enrico, how=\"inner\", on=merge_on_columns)\n",
    "df_contains_matched = computeContainsRelationMatchByEventOrState(df_marco_100, df_enrico)\n",
    "\n",
    "exaxt_match = df_contains_matched[df_contains_matched.MatchedInfo == 'Exact Match'].shape[0]\n",
    "contains_relation_match = df_contains_matched[df_contains_matched.MatchedInfo == 'Contains Relation Match'].shape[0]\n",
    "\n",
    "#precision = (df_exact_matched.shape[0])/(df_enrico.shape[0])\n",
    "#recall = (df_exact_matched.shape[0])/(df_marco_100.shape[0])\n",
    "#f_score = (2*precision*recall)/(precision+recall)\n",
    "#t.add_row(['Annoations of Fist Annotator', 'Exact Match', precision, recall, df_exact_matched.shape[0], 0, 'ALL ELEMENT', f_score])\n",
    "\n",
    "precision = (df_contains_matched.shape[0])/(df_enrico.shape[0])\n",
    "recall = (df_contains_matched.shape[0])/(df_marco_100.shape[0])\n",
    "f_score = (2*precision*recall)/(precision+recall)\n",
    "t.add_row(['Annoations of Fist Annotator', 'Contains Relation Match', precision, recall, exaxt_match, contains_relation_match, 'EVENT or STATE', f_score])\n",
    "\n",
    "#precision = (df_exact_matched.shape[0])/(df_marco_100.shape[0])\n",
    "#recall = (df_exact_matched.shape[0])/(df_enrico.shape[0])\n",
    "#f_score = (2*precision*recall)/(precision+recall)\n",
    "#t.add_row(['Annoations of fourth annotator', 'Exact Match', precision, recall, df_exact_matched.shape[0], 0, 'ALL ELEMENT', f_score])\n",
    "\n",
    "precision = (df_contains_matched.shape[0])/(df_marco_100.shape[0])\n",
    "recall = (df_contains_matched.shape[0])/(df_enrico.shape[0])\n",
    "f_score = (2*precision*recall)/(precision+recall)\n",
    "t.add_row(['Annoations of fourth annotator', 'Contains Relation Match', precision, recall, exaxt_match, contains_relation_match, 'EVENT or STATE', f_score])\n",
    "\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall X Category Element, with Precision = TP/(TP + FP) e Recall = Precision = TP/(TP + FN).\n",
    "## TP: True Positive, FP: False Positive e FN: False Negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+----------------------+--------------------+--------------------+--------------+\n",
      "|        ANNOTATIONS GOLD        | Precision Evaluation | Recall Evaluation  |      F-score       | Type Element |\n",
      "+--------------------------------+----------------------+--------------------+--------------------+--------------+\n",
      "| Annoations of First Annotator  |  0.896551724137931   | 0.9285714285714286 | 0.912280701754386  |     TIME     |\n",
      "| Annoations of Second Annotator |  0.9642857142857143  | 0.9310344827586207 | 0.9473684210526316 |     TIME     |\n",
      "| Annoations of First Annotator  |  0.8675496688741722  | 0.9357142857142857 | 0.9003436426116839 |  WRITER-AG   |\n",
      "| Annoations of Second Annotator |         0.95         | 0.8807947019867549 | 0.9140893470790377 |  WRITER-AG   |\n",
      "| Annoations of First Annotator  |  0.7786259541984732  | 0.9026548672566371 | 0.8360655737704918 |    EVENT     |\n",
      "| Annoations of Second Annotator |  0.9026548672566371  | 0.7786259541984732 | 0.8360655737704918 |    EVENT     |\n",
      "| Annoations of First Annotator  |         0.8          | 0.7796610169491526 | 0.7896995708154507 |     ORG      |\n",
      "| Annoations of Second Annotator |  0.847457627118644   | 0.8695652173913043 | 0.8583690987124463 |     ORG      |\n",
      "| Annoations of First Annotator  |  0.7674418604651163  | 0.7333333333333333 | 0.7499999999999999 |     LOC      |\n",
      "| Annoations of Second Annotator |  0.9222222222222223  | 0.9651162790697675 | 0.9431818181818181 |     LOC      |\n",
      "| Annoations of First Annotator  |  0.9090909090909091  | 0.9090909090909091 | 0.9090909090909091 |  ASP-EVENT   |\n",
      "| Annoations of Second Annotator |  0.9090909090909091  | 0.9090909090909091 | 0.9090909090909091 |  ASP-EVENT   |\n",
      "| Annoations of First Annotator  |         0.7          | 0.7549019607843137 | 0.7264150943396225 |    STATE     |\n",
      "| Annoations of Second Annotator |  0.696078431372549   | 0.6454545454545455 | 0.6698113207547169 |    STATE     |\n",
      "| Annoations of First Annotator  |  0.8367346938775511  | 0.9111111111111111 | 0.8723404255319148 |  WRITER-PA   |\n",
      "| Annoations of Second Annotator |  0.9333333333333333  | 0.8571428571428571 | 0.8936170212765957 |  WRITER-PA   |\n",
      "| Annoations of First Annotator  |         0.8          |        0.8         | 0.8000000000000002 |  REP-EVENT   |\n",
      "| Annoations of Second Annotator |         0.8          |        0.8         | 0.8000000000000002 |  REP-EVENT   |\n",
      "+--------------------------------+----------------------+--------------------+--------------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "# Between First annotator and Second annotator \n",
    "table = PrettyTable(['ANNOTATIONS GOLD', 'Precision Evaluation', 'Recall Evaluation','F-score', 'Type Element',])\n",
    "for element_type in element_types :\n",
    "    df_annot1 = pd.read_csv(annotations_marco_200_csv_path)\n",
    "    df_annot2 = pd.read_csv(annotations_ousmane_200_csv_path)\n",
    "    computed = precisionAndRecallXCategory(df_annot1, df_annot2, element_type)\n",
    "    precision = computed['true_pos']/computed['retrieved'] if computed['retrieved'] > 0 else 'Division By Zero'\n",
    "    recall = computed['true_pos']/computed['relevant'] if computed['relevant'] > 0 else  'Division By Zero'\n",
    "    f_score = (2*precision*recall)/(precision+recall) if type(precision) != str and type(recall) != str else -1\n",
    "    if f_score > 0 :\n",
    "        table.add_row(['Annoations of First Annotator', precision, recall, f_score, element_type])\n",
    "    computed = precisionAndRecallXCategory(df_annot2, df_annot1, element_type)\n",
    "    precision = computed['true_pos']/computed['retrieved'] if computed['retrieved'] > 0 else 'Division By Zero'\n",
    "    recall = computed['true_pos']/computed['relevant'] if computed['relevant'] > 0 else  'Division By Zero'\n",
    "    f_score = (2*precision*recall)/(precision+recall) if type(precision) != str and type(recall) != str else -1\n",
    "    if f_score > 0 :\n",
    "        table.add_row(['Annoations of Second Annotator', precision, recall, f_score, element_type])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+----------------------+--------------------+---------------------+--------------+\n",
      "|        ANNOTATIONS GOLD       | Precision Evaluation | Recall Evaluation  |       F-score       | Type Element |\n",
      "+-------------------------------+----------------------+--------------------+---------------------+--------------+\n",
      "| Annoations of First Annotator |  0.8602150537634409  | 0.851063829787234  |  0.8556149732620322 |     TIME     |\n",
      "| Annoations of Third Annotator |  0.8723404255319149  | 0.8817204301075269 |  0.877005347593583  |     TIME     |\n",
      "| Annoations of First Annotator |  0.8896551724137931  | 0.9347826086956522 |  0.911660777385159  |  WRITER-AG   |\n",
      "| Annoations of Third Annotator |  0.9420289855072463  | 0.896551724137931  |  0.9187279151943464 |  WRITER-AG   |\n",
      "| Annoations of First Annotator |  0.8165680473372781  | 0.8846153846153846 |  0.8492307692307692 |    EVENT     |\n",
      "| Annoations of Third Annotator |  0.8782051282051282  | 0.8106508875739645 |  0.8430769230769232 |    EVENT     |\n",
      "| Annoations of First Annotator |  0.6213592233009708  | 0.6213592233009708 |  0.6213592233009708 |     ORG      |\n",
      "| Annoations of Third Annotator |  0.7475728155339806  | 0.7475728155339806 |  0.7475728155339806 |     ORG      |\n",
      "| Annoations of First Annotator |  0.8428571428571429  | 0.8309859154929577 |  0.8368794326241136 |     LOC      |\n",
      "| Annoations of Third Annotator |  0.6901408450704225  |        0.7         |  0.6950354609929078 |     LOC      |\n",
      "| Annoations of First Annotator |         1.0          |        1.0         |         1.0         |  ASP-EVENT   |\n",
      "| Annoations of Third Annotator |         1.0          |        1.0         |         1.0         |  ASP-EVENT   |\n",
      "| Annoations of First Annotator |  0.5934065934065934  | 0.7012987012987013 |  0.6428571428571428 |    STATE     |\n",
      "| Annoations of Third Annotator |  0.6493506493506493  | 0.5494505494505495 |  0.5952380952380952 |    STATE     |\n",
      "| Annoations of First Annotator |  0.8939393939393939  |      0.921875      |  0.9076923076923077 |  WRITER-PA   |\n",
      "| Annoations of Third Annotator |       0.90625        | 0.8787878787878788 |  0.8923076923076922 |  WRITER-PA   |\n",
      "| Annoations of First Annotator |        0.375         | 0.3333333333333333 | 0.35294117647058826 |  REP-EVENT   |\n",
      "| Annoations of Third Annotator |  0.2222222222222222  |        0.25        | 0.23529411764705882 |  REP-EVENT   |\n",
      "+-------------------------------+----------------------+--------------------+---------------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "# Between First annotator and Third annotator \n",
    "table = PrettyTable(['ANNOTATIONS GOLD', 'Precision Evaluation', 'Recall Evaluation', 'F-score', 'Type Element',])\n",
    "for element_type in element_types :\n",
    "    df_annot1 = pd.read_csv(annotations_marco_200_2_csv_path)\n",
    "    df_annot2 = pd.read_csv(annotations_rossana_200_csv_path)\n",
    "    computed = precisionAndRecallXCategory(df_annot1, df_annot2, element_type)\n",
    "    precision = computed['true_pos']/computed['retrieved'] if computed['retrieved'] > 0 else 'Division By Zero'\n",
    "    recall = computed['true_pos']/computed['relevant'] if computed['relevant'] > 0 else  'Division By Zero'\n",
    "    f_score = (2*precision*recall)/(precision+recall) if type(precision) != str and type(recall) != str else -1\n",
    "    if f_score > 0 :\n",
    "        table.add_row(['Annoations of First Annotator', precision, recall, f_score, element_type])\n",
    "    computed = precisionAndRecallXCategory(df_annot2, df_annot1, element_type)\n",
    "    precision = computed['true_pos']/computed['retrieved'] if computed['retrieved'] > 0 else 'Division By Zero'\n",
    "    recall = computed['true_pos']/computed['relevant'] if computed['relevant'] > 0 else  'Division By Zero'\n",
    "    f_score = (2*precision*recall)/(precision+recall) if type(precision) != str and type(recall) != str else -1\n",
    "    if f_score > 0 :\n",
    "        table.add_row(['Annoations of Third Annotator', precision, recall, f_score, element_type])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+----------------------+---------------------+--------------------+--------------+\n",
      "|        ANNOTATIONS GOLD        | Precision Evaluation |  Recall Evaluation  |      F-score       | Type Element |\n",
      "+--------------------------------+----------------------+---------------------+--------------------+--------------+\n",
      "| Annoations of First Annotator  |  0.9444444444444444  |  0.9444444444444444 | 0.9444444444444444 |     TIME     |\n",
      "| Annoations of Fourth Annotator |  0.9444444444444444  |  0.9444444444444444 | 0.9444444444444444 |     TIME     |\n",
      "| Annoations of First Annotator  |  0.8970588235294118  |  0.9384615384615385 | 0.9172932330827067 |  WRITER-AG   |\n",
      "| Annoations of Fourth Annotator |  0.9538461538461539  |  0.9117647058823529 | 0.9323308270676691 |  WRITER-AG   |\n",
      "| Annoations of First Annotator  |  0.7808219178082192  |  0.8507462686567164 | 0.8142857142857143 |    EVENT     |\n",
      "| Annoations of Fourth Annotator |  0.835820895522388   |  0.7671232876712328 |        0.8         |    EVENT     |\n",
      "| Annoations of First Annotator  |         0.38         |  0.3958333333333333 | 0.3877551020408163 |     ORG      |\n",
      "| Annoations of Fourth Annotator |        0.9375        |         0.9         | 0.9183673469387755 |     ORG      |\n",
      "| Annoations of First Annotator  |  0.7291666666666666  |  0.7777777777777778 | 0.7526881720430108 |     LOC      |\n",
      "| Annoations of Fourth Annotator |  0.9555555555555556  |  0.8958333333333334 | 0.924731182795699  |     LOC      |\n",
      "| Annoations of First Annotator  |  0.6491228070175439  |  0.8409090909090909 | 0.7326732673267327 |    STATE     |\n",
      "| Annoations of Fourth Annotator |  0.7045454545454546  |  0.543859649122807  | 0.613861386138614  |    STATE     |\n",
      "| Annoations of First Annotator  |  0.9090909090909091  |        0.9375       | 0.923076923076923  |  WRITER-PA   |\n",
      "| Annoations of Fourth Annotator |       0.96875        |  0.9393939393939394 | 0.9538461538461539 |  WRITER-PA   |\n",
      "| Annoations of First Annotator  |         0.5          | 0.42857142857142855 | 0.4615384615384615 |  REP-EVENT   |\n",
      "| Annoations of Fourth Annotator | 0.42857142857142855  |         0.5         | 0.4615384615384615 |  REP-EVENT   |\n",
      "+--------------------------------+----------------------+---------------------+--------------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "# Between First annotator and Fourth annotator \n",
    "table = PrettyTable(['ANNOTATIONS GOLD', 'Precision Evaluation', 'Recall Evaluation', 'F-score', 'Type Element'])\n",
    "for element_type in element_types :\n",
    "    df_annot1 = pd.read_csv(annotations_marco_100_csv_path)\n",
    "    df_annot2 = pd.read_csv(annotations_enrico_100_csv_path)\n",
    "    df_annot2['ASP-EVENT'] = np.nan\n",
    "    computed = precisionAndRecallXCategory(df_annot1, df_annot2, element_type)\n",
    "    precision = computed['true_pos']/computed['retrieved'] if computed['retrieved'] > 0 else 'Division By Zero'\n",
    "    recall = computed['true_pos']/computed['relevant'] if computed['relevant'] > 0 else  'Division By Zero'\n",
    "    f_score = (2*precision*recall)/(precision+recall) if type(precision) != str and type(recall) != str else -1\n",
    "    if f_score > 0 :\n",
    "        table.add_row(['Annoations of First Annotator', precision, recall, f_score, element_type])\n",
    "    computed = precisionAndRecallXCategory(df_annot2, df_annot1, element_type)\n",
    "    precision = computed['true_pos']/computed['retrieved'] if computed['retrieved'] > 0 else 'Division By Zero'\n",
    "    recall = computed['true_pos']/computed['relevant'] if computed['relevant'] > 0 else  'Division By Zero'\n",
    "    f_score = (2*precision*recall)/(precision+recall) if type(precision) != str and type(recall) != str else -1\n",
    "    if f_score > 0 :\n",
    "        table.add_row(['Annoations of Fourth Annotator', precision, recall, f_score, element_type])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of annotations over 1000 sentences:  1795\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "verbs = {\n",
    "    'states': {},\n",
    "    'events': {}\n",
    "}\n",
    "df_marco = pd.read_csv(annotations_marco_1000_csv_path).replace(np.nan, '', regex=True)\n",
    "print(\"Number of annotations over 1000 sentences: \", df_marco.shape[0])\n",
    "\n",
    "for index, row in df_marco.iterrows() :\n",
    "    state = str(row[\"STATE\"]).strip().lower()\n",
    "    event = str(row[\"EVENT\"]).strip().lower()\n",
    "    rep_event = str(row[\"REP-EVENT\"]).strip().lower()\n",
    "    asp_event = str(row[\"ASP-EVENT\"]).strip().lower()\n",
    "    location = str(row[\"LOC\"]).strip()\n",
    "    orgaization = str(row[\"ORG\"]).strip()\n",
    "    text = str(row['text'])\n",
    "    is_verb = True\n",
    "    key_verbs = verbs['states'] if len(event.strip()) == 0 else verbs['events']\n",
    "    verb_state_ = state if len(event.strip()) == 0 else event\n",
    "    doc1 = nlp(location)\n",
    "    doc2 = nlp(orgaization)\n",
    "    entities1 = [ent.label_ for ent in doc1.ents]\n",
    "    entities2 = [ent.label_ for ent in doc2.ents]\n",
    "    verb_states = [w for w in word_tokenize(verb_state_) if not w in stop_words]\n",
    "    for verb_state in verb_states :\n",
    "        best_sense = lesk(word_tokenize(text), verb_state, pos=wn.VERB)\n",
    "        if best_sense is None :\n",
    "            best_sense = lesk(word_tokenize(text), verb_state)\n",
    "            is_verb = False\n",
    "        definition = best_sense.definition() if best_sense is not None else verb_state\n",
    "        best_sense = best_sense.name() if best_sense is not None else verb_state\n",
    "        if best_sense in key_verbs :\n",
    "            key_verbs[best_sense]['occorrences'] += 1\n",
    "            key_verbs[best_sense]['rep_event'] += 1 if len(rep_event) > 0 else 0\n",
    "            key_verbs[best_sense]['asp_event'] += 1 if len(asp_event) > 0 else 0 \n",
    "            key_verbs[best_sense]['location'] += 1 if \"GPE\" in entities1 else 0\n",
    "            key_verbs[best_sense]['organization'] += 1 if \"ORG\" in entities2 else 0\n",
    "            if is_verb :\n",
    "                key_verbs[best_sense]['targets'].add(wn_lemmatizer.lemmatize(verb_state, 'v'))\n",
    "            else : \n",
    "                key_verbs[best_sense]['targets'].add(wn_lemmatizer.lemmatize(verb_state))\n",
    "        else :\n",
    "            key_verbs[best_sense] = {}\n",
    "            key_verbs[best_sense]['occorrences'] = 1\n",
    "            key_verbs[best_sense]['definition'] = definition\n",
    "            key_verbs[best_sense]['rep_event'] = 1 if len(rep_event) > 0 else 0\n",
    "            key_verbs[best_sense]['asp_event'] = 1 if len(asp_event) > 0 else 0\n",
    "            key_verbs[best_sense]['location'] = 1 if \"GPE\" in entities1 else 0\n",
    "            key_verbs[best_sense]['organization'] = 1 if \"ORG\" in entities2 else 0\n",
    "            if is_verb :\n",
    "                key_verbs[best_sense]['targets'] = set([wn_lemmatizer.lemmatize(verb_state, 'v')])\n",
    "            else :\n",
    "                key_verbs[best_sense]['targets'] = set([wn_lemmatizer.lemmatize(verb_state)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of annotations (STATE) over 1000 sentences: 677 , sia 37.71587743732591 %.\n",
      "Number of annotations (EVENT) over 1000 sentences: 853 , sia 47.5208913649025 %.\n"
     ]
    }
   ],
   "source": [
    "df_states_quant_analysis = pd.DataFrame(columns=[\n",
    "    'state (WORDNET SENSE IF EXIST)', \n",
    "    'occorrences', \n",
    "    'definition', \n",
    "    'rep_event', \n",
    "    'asp_event', \n",
    "    'location', \n",
    "    'organization', \n",
    "    'targets'\n",
    "])\n",
    "for key, value in verbs['states'].items() :\n",
    "    value_copy = value.copy()\n",
    "    value_copy['state (WORDNET SENSE IF EXIST)'] = key\n",
    "    value_copy['targets'] = \", \".join(value['targets'])\n",
    "    df_states_quant_analysis = df_states_quant_analysis.append(value_copy, ignore_index=True)\n",
    "df_states_quant_analysis = df_states_quant_analysis.sort_values(by='occorrences', ascending=False)\n",
    "print(\n",
    "    \"Number of annotations (STATE) over 1000 sentences:\", \n",
    "    df_states_quant_analysis['occorrences'].sum(),\n",
    "    \", sia\",\n",
    "    (df_states_quant_analysis['occorrences'].sum() * 100) / df_marco.shape[0],\n",
    "    \"%.\"\n",
    ")\n",
    "df_states_quant_analysis.to_csv(states_quantitative_analysis_csv_path, index=False)\n",
    "\n",
    "df_events_quant_analysis = pd.DataFrame(columns=[\n",
    "    'event (WORDNET SENSE IF EXIST)', \n",
    "    'occorrences', \n",
    "    'definition', \n",
    "    'rep_event', \n",
    "    'asp_event', \n",
    "    'location', \n",
    "    'organization', \n",
    "    'targets'\n",
    "])\n",
    "for key, value in verbs['events'].items() :\n",
    "    value_copy = value.copy()\n",
    "    value_copy['event (WORDNET SENSE IF EXIST)'] = key\n",
    "    value_copy['targets'] = \", \".join(value['targets'])\n",
    "    df_events_quant_analysis = df_events_quant_analysis.append(value_copy, ignore_index=True)\n",
    "df_events_quant_analysis = df_events_quant_analysis.sort_values(by='occorrences', ascending=False)\n",
    "print(\n",
    "    \"Number of annotations (EVENT) over 1000 sentences:\", \n",
    "    df_events_quant_analysis['occorrences'].sum(),\n",
    "    \", sia\",\n",
    "    (df_events_quant_analysis['occorrences'].sum() * 100) / df_marco.shape[0],\n",
    "    \"%.\"\n",
    ")\n",
    "df_events_quant_analysis.to_csv(events_quantitative_analysis_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg1 = df_states_quant_analysis.groupby('targets').agg({\n",
    "    'state (WORDNET SENSE IF EXIST)': ' ## '.join, \n",
    "    'occorrences': sum, \n",
    "    'definition': ' ## '.join, \n",
    "    'rep_event': sum, \n",
    "    'asp_event': sum, \n",
    "    'location': sum, \n",
    "    'organization': sum,\n",
    "    'targets': ' ## '.join\n",
    "})\n",
    "df_agg1 = df_agg1.sort_values(by='occorrences', ascending=False)\n",
    "df_agg1.to_csv(states_quantitative_analysis_groupby_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg2 = df_events_quant_analysis.groupby('targets').agg({\n",
    "    'event (WORDNET SENSE IF EXIST)': ' ## '.join, \n",
    "    'occorrences': sum, \n",
    "    'definition': ' ## '.join, \n",
    "    'rep_event': sum, \n",
    "    'asp_event': sum, \n",
    "    'location': sum, \n",
    "    'organization': sum,\n",
    "    'targets': ' ## '.join\n",
    "})\n",
    "df_agg2 = df_agg2.sort_values(by='occorrences', ascending=False)\n",
    "df_agg2.to_csv(events_quantitative_analysis_groupby_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('receive.v.02') receive a specified treatment (abstract)\n",
      "Synset('receive.v.02') receive a specified treatment (abstract)\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Ramachandran, whose father wanted him to become a physician rather than a researcher, obtained an M.B.B.S. from Stanley Medical College in Chennai, India.\"\n",
    "sense = lesk(word_tokenize(sentence), 'obtained', pos=wn.VERB)\n",
    "print(sense, sense.definition())\n",
    "\n",
    "sentence = \"He received the Kerala Sahitya Academi award in the `poetry' section for his collection, Nellickal Muraleedharante Kavithakal in 2004.\"\n",
    "sense = lesk(word_tokenize(sentence), 'obtained', pos=wn.VERB)\n",
    "print(sense, sense.definition())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9c0098ef03c01875785b14fb83d5161d4a7df326e3b00d12d4d2605823f1c481"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
