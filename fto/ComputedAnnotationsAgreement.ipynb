{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Customer Variables\n",
    "annotations_ousmane_200_json_path = \"inputs\\\\annotations-ousmane-200.json\" \n",
    "annotations_ousmane_200_csv_path = \"inputs\\\\annotations-ousmane-200.csv\"\n",
    "annotations_marco_1000_csv_path = \"inputs\\\\1000annotazioni.csv\"\n",
    "annotations_marco_200_csv_path = \"inputs\\\\annotations-marco-200.csv\"\n",
    "annotations_matched_200_csv_path = \"outputs\\\\annotations-matched-200.csv\"\n",
    "annotations_contains_matched_200_csv_path = \"outputs\\\\annotations_contains-matched-200.csv\"\n",
    "states_quantitative_analysis_csv_path = \"outputs\\\\states_quantitative_analysis.csv\"\n",
    "events_quantitative_analysis_csv_path = \"outputs\\\\events_quantitative_analysis.csv\"\n",
    "states_quantitative_analysis_groupby_lemmas = \"outputs\\\\states_quantitative_analysis_groupby_lemmas.csv\"\n",
    "events_quantitative_analysis_groupby_lemmas = \"outputs\\\\events_quantitative_analysis_groupby_lemmas.csv\"\n",
    "test_csv_path = \"outputs\\\\test.csv\"\n",
    "exact_agreement_columns = [\"author\", \"sent_id\", \"text\", \"TIME\", \"WRITER-AG\", \"EVENT\", \"ORG\", \"LOC\", \"ASP-EVENT\", \"STATE\", \"WRITER-PA\", \"REP-EVENT\"]\n",
    "soft_agreement_columns = [\"author\", \"sent_id\", \"text\", \"EVENT\", \"STATE\"]\n",
    "merge_by_columns = [\"author\", \"sent_id\", \"text\"]\n",
    "element_types_without_events_and_states = [\"WRITER-AG\", \"ORG\", \"LOC\", \"ASP-EVENT\", \"WRITER-PA\", \"REP-EVENT\"]\n",
    "\n",
    "## Libraries Variables\n",
    "stop_words = list(get_stop_words('en'))         #About 900 stopwords\n",
    "nltk_words = list(stopwords.words('english')) #About 150 stopwords\n",
    "stop_words.extend(nltk_words)\n",
    "wn_lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function convert the annotation file json in a annotation file csv.\n",
    "def annotations_json_to_csv(path_file_json, annotator_name, path_file_csv) :\n",
    "    df = pd.DataFrame()\n",
    "    with open(path_file_json, \"r\", encoding='utf-8') as f:\n",
    "        file_contents = json.load(f)\n",
    "        for item in file_contents:\n",
    "            for annotation in item['annotations']:\n",
    "                container = dict()\n",
    "                container['annotator'] = annotator_name\n",
    "                container['author'] = item['data']['author']\n",
    "                container['sent_id'] = item['data']['sent_id']\n",
    "                container['text'] = item['data']['text']\n",
    "                for v in annotation['result']:\n",
    "                    container[v['value']['labels'][0]] = v['value']['text']\n",
    "                df = df.append(container, ignore_index=True)\n",
    "    df.to_csv(path_file_csv, index=False)\n",
    "\n",
    "# This function verify if string elements of two list are \"contains relations\".\n",
    "# \"contains relations\": for example we have two string str1 and str2, if str1 is sub strinf of str2\n",
    "# or str2 is sub string of str1\n",
    "def checkContainsRelations(items_1, items_2) :\n",
    "    computed = True\n",
    "    if len(items_1) != len(items_2) : return False\n",
    "    for index in range(len(items_1)) : \n",
    "        #computed = computed and (str(items_1[index]) in str(items_2[index]) or str(items_2[index]) in str(items_1[index]))\n",
    "        sub_compute = True\n",
    "        lx = str(items_1[index]).split()\n",
    "        ly = str(items_2[index]).split()\n",
    "        if len(lx) > len(ly) :\n",
    "            lx, ly = swapedItems(lx, ly)\n",
    "        for i in range(len(lx)) :\n",
    "            sub_compute = sub_compute and (lx[i] in ly)\n",
    "            if not sub_compute :\n",
    "                break\n",
    "        computed = computed and sub_compute\n",
    "        if not computed : return False\n",
    "    return computed\n",
    "\n",
    "# Contain relation between two string\n",
    "def checkContainRelation(item_1, item_2) :\n",
    "    computed = True\n",
    "    lx = str(item_1).split()\n",
    "    ly = str(item_2).split()\n",
    "    if len(lx) > len(ly) :\n",
    "        lx, ly = swapedItems(lx, ly)\n",
    "    for i in range(len(lx)) :\n",
    "        computed = computed and (lx[i] in ly)\n",
    "        if not computed :\n",
    "            return False\n",
    "    return computed\n",
    "\n",
    "\n",
    "## This function merge two csv with Pandas\n",
    "def merged_two_csv(path_csv1, path_csv2, how_mode, onColumns) :\n",
    "    df1 = pd.read_csv(path_csv1)\n",
    "    df2 = pd.read_csv(path_csv2)\n",
    "    df = pd.merge(df1, df2, how=how_mode, on=onColumns) \n",
    "    return df\n",
    "\n",
    "def concateItems(item_1, item_2) :\n",
    "    item = \"Value1: \"\n",
    "    item += str(item_1) if str(item_1) != \"nan\" else \"MISSING_VALUE\"\n",
    "    item += \" & Value2: \"\n",
    "    item += str(item_2) if str(item_2) != \"nan\" else \"MISSING_VALUE\"\n",
    "    return item\n",
    "\n",
    "def swapedItems(item_1, item_2) :\n",
    "    temp = item_1\n",
    "    item_1 = item_2\n",
    "    item_2 = temp\n",
    "    return item_1,item_2\n",
    "\n",
    "## This function take two annotations Dataframe and compute the Agreement by Category\n",
    "def computeAgreementByElementType(df_annotations1, df_annotations2, element_type, output_file_name) : \n",
    "    \n",
    "    # Create a matched Dictionnary container\n",
    "    matched_container = {\n",
    "        'Author': [],\n",
    "        'SentenceId': [],\n",
    "        element_type: [],\n",
    "        'MatchedInfo': [],\n",
    "        'Text': []\n",
    "    }\n",
    "\n",
    "    if element_type not in element_types_without_events_and_states :\n",
    "        raise Exception(\"Element Type not exist or has type EVENT or STATE!\")\n",
    "\n",
    "    # Compute Pivot Set\n",
    "    df_set_pivot = df_annotations1[[\"author\", \"sent_id\", \"text\"]].drop_duplicates(subset=[\"author\", \"sent_id\", \"text\"])\n",
    "\n",
    "    for _, row in df_set_pivot.iterrows() :\n",
    "        author = row[\"author\"]\n",
    "        sent_id = row[\"sent_id\"]\n",
    "        text = row[\"text\"]\n",
    "        df_x = df_annotations1.loc[(df_annotations1['author'] == author) & (df_annotations1['sent_id'] == sent_id) & (df_annotations1['text'] == text)]\n",
    "        df_y = df_annotations2.loc[(df_annotations2['author'] == author) & (df_annotations2['sent_id'] == sent_id) & (df_annotations2['text'] == text)]\n",
    "        if (df_x.shape[0] < df_y.shape[0]) :\n",
    "            df_x, df_y = swapedItems(df_x, df_y)\n",
    "        for _, row_x in df_x.iterrows() :\n",
    "            items_1 = [row_x[element_type]]\n",
    "            for _, row_y in df_y.iterrows() :\n",
    "                items_2 = [row_y[element_type]]\n",
    "                if items_1 == items_2 : \n",
    "                    matched_container['Author'].append(author)\n",
    "                    matched_container['SentenceId'].append(sent_id)\n",
    "                    matched_container['Text'].append(text)\n",
    "                    matched_container[element_type].append(row_x[element_type])\n",
    "                    matched_container['MatchedInfo'].append(\"Exact Match\")\n",
    "                    break\n",
    "                elif checkContainsRelations(items_1, items_2) :\n",
    "                    matched_container['Author'].append(author)\n",
    "                    matched_container['SentenceId'].append(sent_id)\n",
    "                    matched_container['Text'].append(text)\n",
    "                    if str(row_x[element_type]) != \"nan\" or str(row_y[element_type]) != \"nan\" : \n",
    "                        matched_container[element_type].append(concateItems(row_x[element_type],row_y[element_type]))\n",
    "                    else : matched_container[element_type].append(\"\")\n",
    "                    matched_container['MatchedInfo'].append(\"Contains Relation Match\")\n",
    "                    break\n",
    "        \n",
    "    # Convert the dictionary into DataFrame\n",
    "    df_contains_matched = pd.DataFrame(matched_container, columns = [element_type, 'MatchedInfo', 'SentenceId', 'Text'])\n",
    "    df_contains_matched.replace(np.nan, '', regex=True)\n",
    "    df_contains_matched.to_csv(output_file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trnasform Annotions file JSON to file CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_json_to_csv(annotations_ousmane_200_json_path, \"ousmane\", annotations_ousmane_200_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute exact Agreement \n",
    "### Output file => \"outputs\\\\annotations-matched-200.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exact_matched = merged_two_csv(annotations_ousmane_200_csv_path, annotations_marco_1000_csv_path, \"inner\", exact_agreement_columns)\n",
    "df_exact_matched.to_csv(annotations_matched_200_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute soft Agreement (columns values are contains relations)\n",
    "### Output file => \"outputs\\\\annotations_contains-matched-200.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Both df of annotatore and  make Pivot df\n",
    "df_ousmane = pd.read_csv(annotations_ousmane_200_csv_path)\n",
    "df_marco = pd.read_csv(annotations_marco_1000_csv_path)\n",
    "df_set_pivot = df_ousmane[[\"author\", \"sent_id\", \"text\"]].drop_duplicates(subset=[\"author\", \"sent_id\", \"text\"])\n",
    "\n",
    "# Create a matched Dictionnary container\n",
    "matched_container = {\n",
    "    'Author': [],\n",
    "    'SentenceId': [],\n",
    "    'EVENT': [],\n",
    "    'STATE': [],\n",
    "    'MatchedInfo': [],\n",
    "    'Text': []\n",
    "}\n",
    "\n",
    "# Compute match on remain elements types\n",
    "other_matched_container = {\n",
    "    'WRITER-AG': {'exact_match': 0, 'contain_match': 0},\n",
    "    'WRITER-PA': {'exact_match': 0, 'contain_match': 0},  \n",
    "    'ORG': {'exact_match': 0, 'contain_match': 0}, \n",
    "    'LOC': {'exact_match': 0, 'contain_match': 0}, \n",
    "    'ASP-EVENT': {'exact_match': 0, 'contain_match': 0},\n",
    "    'REP-EVENT': {'exact_match': 0, 'contain_match': 0}\n",
    "}\n",
    "\n",
    "df_marco_200 = pd.DataFrame(columns=list(df_marco.columns))\n",
    "\n",
    "# Iterate Pivot df and Processing soft matching on Some columns \n",
    "for index, row in df_set_pivot.iterrows():\n",
    "    author = row[\"author\"]\n",
    "    sent_id = row[\"sent_id\"]\n",
    "    text = row[\"text\"]\n",
    "    df_x = df_ousmane.loc[(df_ousmane['author'] == author) & (df_ousmane['sent_id'] == sent_id) & (df_ousmane['text'] == text)]\n",
    "    df_y = df_marco.loc[(df_marco['author'] == author) & (df_marco['sent_id'] == sent_id) & (df_marco['text'] == text)]\n",
    "    df_marco_200 = df_y if len(df_marco_200) == 0 else pd.concat([df_marco_200, df_y], ignore_index=True)\n",
    "    if (df_x.shape[0] < df_y.shape[0]) :\n",
    "        df_x, df_y = swapedItems(df_x, df_y)\n",
    "    for index_x, row_x in df_x.iterrows() :\n",
    "        items_1 = [row_x[\"EVENT\"], row_x[\"STATE\"]]\n",
    "        for index_y, row_y in df_y.iterrows() :\n",
    "            items_2 = [row_y[\"EVENT\"], row_y[\"STATE\"]]\n",
    "            if items_1 == items_2 : \n",
    "                matched_container['Author'].append(author)\n",
    "                matched_container['SentenceId'].append(sent_id)\n",
    "                matched_container['Text'].append(text)\n",
    "                matched_container['EVENT'].append(row_x[\"EVENT\"])\n",
    "                matched_container['STATE'].append(row_x[\"STATE\"])\n",
    "                matched_container['MatchedInfo'].append(\"Exact Match\")\n",
    "                break\n",
    "            elif checkContainsRelations(items_1, items_2) :\n",
    "                matched_container['Author'].append(author)\n",
    "                matched_container['SentenceId'].append(sent_id)\n",
    "                matched_container['Text'].append(text)\n",
    "                if str(row_x[\"EVENT\"]) != \"nan\" or str(row_y[\"EVENT\"]) != \"nan\" : \n",
    "                    matched_container['EVENT'].append(concateItems(row_x[\"EVENT\"],row_y[\"EVENT\"]))\n",
    "                else : matched_container['EVENT'].append(\"\")\n",
    "                if str(row_x[\"STATE\"]) != \"nan\" or str(row_y[\"STATE\"]) != \"nan\" : \n",
    "                    matched_container['STATE'].append(concateItems(row_x[\"STATE\"], row_y[\"STATE\"]))\n",
    "                else : matched_container['STATE'].append(\"\")\n",
    "                matched_container['MatchedInfo'].append(\"Contains Relation Match\")\n",
    "                break\n",
    "\n",
    "# Convert the dictionary into DataFrame\n",
    "df_contains_matched = pd.DataFrame(matched_container, columns = ['EVENT', 'STATE', 'MatchedInfo', 'SentenceId', 'Text'])\n",
    "df_contains_matched.replace(np.nan, '', regex=True)\n",
    "df_contains_matched.to_csv(annotations_contains_matched_200_csv_path, index=False)\n",
    "df_marco_200.to_csv(annotations_marco_200_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-------------------------+----------------------+---------------------+\n",
      "|         GOLD ANNOTATIONS        |    Intersection Mode    | Precision Evaluation |  Recall Evaluation  |\n",
      "+---------------------------------+-------------------------+----------------------+---------------------+\n",
      "|  Annoations for Fist Annotator  |       Exact Match       |  0.3333333333333333  | 0.27823691460055094 |\n",
      "|  Annoations for Fist Annotator  | Contains Relation Match |  0.7095709570957096  |  0.5922865013774105 |\n",
      "| Annoations for Second Annotator |       Exact Match       | 0.27823691460055094  |  0.3333333333333333 |\n",
      "| Annoations for Second Annotator | Contains Relation Match |  0.5922865013774105  |  0.7095709570957096 |\n",
      "+---------------------------------+-------------------------+----------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t = PrettyTable(['GOLD ANNOTATIONS', 'Intersection Mode', 'Precision Evaluation', 'Recall Evaluation'])\n",
    "\n",
    "precision = (df_exact_matched.shape[0])/(df_ousmane.shape[0])\n",
    "recall = (df_exact_matched.shape[0])/(df_marco_200.shape[0])\n",
    "t.add_row(['Annoations for Fist Annotator', 'Exact Match', precision, recall])\n",
    "\n",
    "precision = (df_contains_matched.shape[0])/(df_ousmane.shape[0])\n",
    "recall = (df_contains_matched.shape[0])/(df_marco_200.shape[0])\n",
    "t.add_row(['Annoations for Fist Annotator', 'Contains Relation Match', precision, recall])\n",
    "\n",
    "precision = (df_exact_matched.shape[0])/(df_marco_200.shape[0])\n",
    "recall = (df_exact_matched.shape[0])/(df_ousmane.shape[0])\n",
    "t.add_row(['Annoations for Second Annotator', 'Exact Match', precision, recall])\n",
    "\n",
    "precision = (df_contains_matched.shape[0])/(df_marco_200.shape[0])\n",
    "recall = (df_contains_matched.shape[0])/(df_ousmane.shape[0])\n",
    "t.add_row(['Annoations for Second Annotator', 'Contains Relation Match', precision, recall])\n",
    "\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of annotations over 1000 sentences:  1795\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "verbs = {\n",
    "    'states': {},\n",
    "    'events': {}\n",
    "}\n",
    "\n",
    "df_marco = df_marco.replace(np.nan, '', regex=True)\n",
    "print(\"Number of annotations over 1000 sentences: \", df_marco.shape[0])\n",
    "\n",
    "for index, row in df_marco.iterrows() :\n",
    "    state = str(row[\"STATE\"]).strip().lower()\n",
    "    event = str(row[\"EVENT\"]).strip().lower()\n",
    "    rep_event = str(row[\"REP-EVENT\"]).strip().lower()\n",
    "    asp_event = str(row[\"ASP-EVENT\"]).strip().lower()\n",
    "    location = str(row[\"LOC\"]).strip()\n",
    "    orgaization = str(row[\"ORG\"]).strip()\n",
    "    text = str(row['text'])\n",
    "    is_verb = True\n",
    "    key_verbs = verbs['states'] if len(event.strip()) == 0 else verbs['events']\n",
    "    verb_state_ = state if len(event.strip()) == 0 else event\n",
    "    doc1 = nlp(location)\n",
    "    doc2 = nlp(orgaization)\n",
    "    entities1 = [ent.label_ for ent in doc1.ents]\n",
    "    entities2 = [ent.label_ for ent in doc2.ents]\n",
    "    verb_states = [w for w in word_tokenize(verb_state_) if not w in stop_words]\n",
    "    for verb_state in verb_states :\n",
    "        best_sense = lesk(word_tokenize(text), verb_state, pos=wn.VERB)\n",
    "        if best_sense is None :\n",
    "            best_sense = lesk(word_tokenize(text), verb_state)\n",
    "            is_verb = False\n",
    "        definition = best_sense.definition() if best_sense is not None else verb_state\n",
    "        best_sense = best_sense.name() if best_sense is not None else verb_state\n",
    "        if best_sense in key_verbs :\n",
    "            key_verbs[best_sense]['occorrences'] += 1\n",
    "            key_verbs[best_sense]['rep_event'] += 1 if len(rep_event) > 0 else 0\n",
    "            key_verbs[best_sense]['asp_event'] += 1 if len(asp_event) > 0 else 0 \n",
    "            key_verbs[best_sense]['location'] += 1 if \"GPE\" in entities1 else 0\n",
    "            key_verbs[best_sense]['organization'] += 1 if \"ORG\" in entities2 else 0\n",
    "            if is_verb :\n",
    "                key_verbs[best_sense]['targets'].add(wn_lemmatizer.lemmatize(verb_state, 'v'))\n",
    "            else : \n",
    "                key_verbs[best_sense]['targets'].add(wn_lemmatizer.lemmatize(verb_state))\n",
    "        else :\n",
    "            key_verbs[best_sense] = {}\n",
    "            key_verbs[best_sense]['occorrences'] = 1\n",
    "            key_verbs[best_sense]['definition'] = definition\n",
    "            key_verbs[best_sense]['rep_event'] = 1 if len(rep_event) > 0 else 0\n",
    "            key_verbs[best_sense]['asp_event'] = 1 if len(asp_event) > 0 else 0\n",
    "            key_verbs[best_sense]['location'] = 1 if \"GPE\" in entities1 else 0\n",
    "            key_verbs[best_sense]['organization'] = 1 if \"ORG\" in entities2 else 0\n",
    "            if is_verb :\n",
    "                key_verbs[best_sense]['targets'] = set([wn_lemmatizer.lemmatize(verb_state, 'v')])\n",
    "            else :\n",
    "                key_verbs[best_sense]['targets'] = set([wn_lemmatizer.lemmatize(verb_state)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of annotations (STATE) over 1000 sentences: 677 , sia 37.71587743732591 %.\n",
      "Number of annotations (EVENT) over 1000 sentences: 853 , sia 47.5208913649025 %.\n"
     ]
    }
   ],
   "source": [
    "df_states_quant_analysis = pd.DataFrame(columns=[\n",
    "    'state (WORDNET SENSE IF EXIST)', \n",
    "    'occorrences', \n",
    "    'definition', \n",
    "    'rep_event', \n",
    "    'asp_event', \n",
    "    'location', \n",
    "    'organization', \n",
    "    'targets'\n",
    "])\n",
    "for key, value in verbs['states'].items() :\n",
    "    value_copy = value.copy()\n",
    "    value_copy['state (WORDNET SENSE IF EXIST)'] = key\n",
    "    value_copy['targets'] = \", \".join(value['targets'])\n",
    "    df_states_quant_analysis = df_states_quant_analysis.append(value_copy, ignore_index=True)\n",
    "df_states_quant_analysis = df_states_quant_analysis.sort_values(by='occorrences', ascending=False)\n",
    "print(\n",
    "    \"Number of annotations (STATE) over 1000 sentences:\", \n",
    "    df_states_quant_analysis['occorrences'].sum(),\n",
    "    \", sia\",\n",
    "    (df_states_quant_analysis['occorrences'].sum() * 100) / df_marco.shape[0],\n",
    "    \"%.\"\n",
    ")\n",
    "df_states_quant_analysis.to_csv(states_quantitative_analysis_csv_path, index=False)\n",
    "\n",
    "df_events_quant_analysis = pd.DataFrame(columns=[\n",
    "    'event (WORDNET SENSE IF EXIST)', \n",
    "    'occorrences', \n",
    "    'definition', \n",
    "    'rep_event', \n",
    "    'asp_event', \n",
    "    'location', \n",
    "    'organization', \n",
    "    'targets'\n",
    "])\n",
    "for key, value in verbs['events'].items() :\n",
    "    value_copy = value.copy()\n",
    "    value_copy['event (WORDNET SENSE IF EXIST)'] = key\n",
    "    value_copy['targets'] = \", \".join(value['targets'])\n",
    "    df_events_quant_analysis = df_events_quant_analysis.append(value_copy, ignore_index=True)\n",
    "df_events_quant_analysis = df_events_quant_analysis.sort_values(by='occorrences', ascending=False)\n",
    "print(\n",
    "    \"Number of annotations (EVENT) over 1000 sentences:\", \n",
    "    df_events_quant_analysis['occorrences'].sum(),\n",
    "    \", sia\",\n",
    "    (df_events_quant_analysis['occorrences'].sum() * 100) / df_marco.shape[0],\n",
    "    \"%.\"\n",
    ")\n",
    "df_events_quant_analysis.to_csv(events_quantitative_analysis_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg1 = df_states_quant_analysis.groupby('targets').agg({\n",
    "    'state (WORDNET SENSE IF EXIST)': ' ## '.join, \n",
    "    'occorrences': sum, \n",
    "    'definition': ' ## '.join, \n",
    "    'rep_event': sum, \n",
    "    'asp_event': sum, \n",
    "    'location': sum, \n",
    "    'organization': sum,\n",
    "    'targets': ' ## '.join\n",
    "})\n",
    "df_agg1 = df_agg1.sort_values(by='occorrences', ascending=False)\n",
    "df_agg1.to_csv(states_quantitative_analysis_groupby_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg2 = df_events_quant_analysis.groupby('targets').agg({\n",
    "    'event (WORDNET SENSE IF EXIST)': ' ## '.join, \n",
    "    'occorrences': sum, \n",
    "    'definition': ' ## '.join, \n",
    "    'rep_event': sum, \n",
    "    'asp_event': sum, \n",
    "    'location': sum, \n",
    "    'organization': sum,\n",
    "    'targets': ' ## '.join\n",
    "})\n",
    "df_agg2 = df_agg2.sort_values(by='occorrences', ascending=False)\n",
    "df_agg2.to_csv(events_quantitative_analysis_groupby_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0027855153203342\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print((18*100)/1795)\n",
    "print(not \"ous\" in [\"ous\", \"mane\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# element_types_without_events_and_states = [\"WRITER-AG\", \"ORG\", \"LOC\", \"ASP-EVENT\", \"WRITER-PA\", \"REP-EVENT\"]\n",
    "computeAgreementByElementType(df_ousmane, df_marco_200, \"WRITER-AG\", test_csv_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9c0098ef03c01875785b14fb83d5161d4a7df326e3b00d12d4d2605823f1c481"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
